{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2273e787-23cb-4c54-a406-a1f313207ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "933a599f-2f54-475e-9acc-cc82b9c911dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_file_columns(file_path, n_rows=5):\n",
    "    \"\"\"\n",
    "    Quickly read the first few rows of a pipe-delimited file\n",
    "    and print cleaned column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        delimiter=\"|\",\n",
    "        nrows=n_rows,\n",
    "        dtype=str\n",
    "    )\n",
    "    # Clean columns\n",
    "    df.columns = df.columns.str.strip(\" ~'\")\n",
    "    print(f\"Columns in {file_path}:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52169717-6cf8-4fc1-b850-0220e60e6a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in ../data/raw/FACTPCRPRIMARYIMPRESSION.txt:\n",
      "- PcrKey\n",
      "- eSituation_11\n",
      "Columns in ../data/raw/FACTPCRSECONDARYIMPRESSION.txt:\n",
      "- PcrKey\n",
      "- eSituation_12\n",
      "Columns in ../data/raw/FACTPCRPRIMARYSYMPTOM.txt:\n",
      "- PcrKey\n",
      "- eSituation_09\n",
      "Columns in ../data/raw/FACTPCRADDITIONALSYMPTOM.txt:\n",
      "- PcrKey\n",
      "- eSituation_10\n",
      "Columns in ../data/raw/FACTPCRCAUSEOFINJURY.txt:\n",
      "- PcrKey\n",
      "- eInjury_01\n",
      "Columns in ../data/raw/FACTPCRINJURYRISKFACTOR.txt:\n",
      "- PcrKey\n",
      "- eInjury_04\n",
      "Columns in ../data/raw/FACTPCRTRAUMACRITERIA.txt:\n",
      "- PcrKey\n",
      "- eInjury_03\n",
      "Columns in ../data/raw/FACTPCRBARRIERTOCARE.txt:\n",
      "- PcrKey\n",
      "- eHistory_01\n",
      "Columns in ../data/raw/FACTPCRDISPATCHDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_08\n",
      "Columns in ../data/raw/FACTPCRRESPONSEDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_09\n",
      "Columns in ../data/raw/FACTPCRSCENEDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_10\n",
      "Columns in ../data/raw/FACTPCRTRANSPORTDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_11\n",
      "Columns in ../data/raw/FACTPCRTURNAROUNDDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_12\n",
      "Columns in ../data/raw/FACTPCRDESTINATIONREASON.txt:\n",
      "- PcrKey\n",
      "- eDisposition_20\n",
      "Columns in ../data/raw/FACTPCRDESTINATIONTEAM.txt:\n",
      "- eDisposition_25\n",
      "- PcrKey\n",
      "- eDisposition_24\n",
      "Columns in ../data/raw/FACTPCRVITAL.txt:\n",
      "- PcrVitalKey\n",
      "- PcrKey\n",
      "- eVitals_06\n",
      "- eVitals_10\n",
      "- eVitals_12\n",
      "- eVitals_14\n",
      "- eVitals_16\n",
      "- eVitals_18\n",
      "- eVitals_27\n",
      "- eVitals_02\n",
      "- eVitals_04\n",
      "- eVitals_08\n",
      "- eVitals_26\n",
      "- eVitals_29\n",
      "- eVitals_30\n",
      "- eVitals_31\n",
      "- eVitals_19\n",
      "- eVitals_20\n",
      "- eVitals_21\n",
      "- eVitals_01\n",
      "Columns in ../data/raw/FACTPCRARRESTCPRPROVIDED.txt:\n",
      "- PcrKey\n",
      "- eArrest_09\n",
      "Columns in ../data/raw/FACTPCRARRESTRESUSCITATION.txt:\n",
      "- PcrKey\n",
      "- eArrest_03\n",
      "Columns in ../data/raw/FACTPCRARRESTRHYTHMDESTINATION.txt:\n",
      "- PcrKey\n",
      "- eArrest_17\n",
      "Columns in ../data/raw/FACTPCRARRESTROSC.txt:\n",
      "- PcrKey\n",
      "- eArrest_12\n",
      "Columns in ../data/raw/FACTPCRARRESTWITNESS.txt:\n",
      "- PcrKey\n",
      "- eArrest_04\n",
      "Columns in ../data/raw/FACTPCRMEDICATION.txt:\n",
      "- eMedications_01\n",
      "- PcrMedicationKey\n",
      "- PcrKey\n",
      "- eMedications_03\n",
      "- eMedications_03Descr\n",
      "- eMedications_05\n",
      "- eMedications_06\n",
      "- eMedications_07\n",
      "- eMedications_10\n",
      "- eMedications_02\n",
      "Columns in ../data/raw/FACTPCRPROCEDURE.txt:\n",
      "- PcrProcedureKey\n",
      "- PcrKey\n",
      "- eProcedures_03\n",
      "- eProcedures_05\n",
      "- eProcedures_06\n",
      "- eProcedures_08\n",
      "- eProcedures_10\n",
      "- eProcedures_02\n",
      "- eProcedures_01\n",
      "Columns in ../data/raw/FACTPCRPROTOCOL.txt:\n",
      "- PcrKey\n",
      "- eProtocols_01\n",
      "- eProtocols_02\n",
      "Columns in ../data/raw/FACTPCRADDITIONALRESPONSEMODE.txt:\n",
      "- PcrKey\n",
      "- eResponse_24\n",
      "Columns in ../data/raw/FACTPCRADDITIONALTRANSPORTMODE.txt:\n",
      "- PcrKey\n",
      "- eDisposition_18\n",
      "Columns in ../data/raw/FACTPCRALCOHOLDRUGUSEINDICATOR.txt:\n",
      "- PcrKey\n",
      "- eHistory_17\n",
      "Columns in ../data/raw/FACTPCRWORKRELATEDEXPOSURE.txt:\n",
      "- PcrKey\n",
      "- eOther_05\n",
      "Columns in ../data/raw/PCRPATIENTRACEGROUP.txt:\n",
      "- PcrPatientRaceGroupKey\n",
      "- PcrKey\n",
      "- ePatient_14\n",
      "Columns in ../data/raw/PCRPROCCOMPGROUP.txt:\n",
      "- PcrProcCompGroupKey\n",
      "- ProcedureKey\n",
      "- eProcedures_07\n",
      "Columns in ../data/raw/PCRMEDCOMPGROUP.txt:\n",
      "- PcrMedCompGroupKey\n",
      "- MedicationKey\n",
      "- eMedications_08\n",
      "Columns in ../data/raw/PCRVITALECGGROUP.txt:\n",
      "- PcrVitalECGGroupKey\n",
      "- VitalKey\n",
      "- eVitals_03\n",
      "Columns in ../data/raw/PCRVITALECGINTERPRETATIONGROUP.txt:\n",
      "- PcrVitalECGInterpretationGroupKe\n",
      "- VitalKey\n",
      "- eVitals_05\n",
      "Columns in ../data/raw/PCRVITALGLASGOWQUALIFIERGROUP.txt:\n",
      "- PcrVitalECGGroupKey\n",
      "- VitalKey\n",
      "- eVitals_22\n",
      "Columns in ../data/raw/pub_pcrevents_cp25.txt:\n",
      "- PcrKey\n",
      "- eDispatch_01\n",
      "- eDispatch_02\n",
      "- eArrest_14\n",
      "- eArrest_01\n",
      "- eArrest_02\n",
      "- eArrest_05\n",
      "- eArrest_07\n",
      "- eArrest_11\n",
      "- eArrest_16\n",
      "- eArrest_18\n",
      "- eDisposition_12\n",
      "- eDisposition_19\n",
      "- eDisposition_16\n",
      "- eDisposition_21\n",
      "- eDisposition_22\n",
      "- eDisposition_23\n",
      "- eOutcome_01\n",
      "- eOutcome_02\n",
      "- ePatient_15\n",
      "- ePatient_16\n",
      "- ePayment_01\n",
      "- ePayment_50\n",
      "- eResponse_05\n",
      "- eResponse_07\n",
      "- eResponse_15\n",
      "- eResponse_23\n",
      "- eScene_01\n",
      "- eScene_06\n",
      "- eScene_07\n",
      "- eScene_08\n",
      "- eScene_09\n",
      "- eSituation_02\n",
      "- eSituation_07\n",
      "- eSituation_08\n",
      "- eSituation_13\n",
      "- eSituation_01\n",
      "- eTimes_01\n",
      "- eTimes_03\n",
      "- eTimes_05\n",
      "- eTimes_06\n",
      "- eTimes_07\n",
      "- eTimes_09\n",
      "- eTimes_11\n",
      "- eTimes_12\n",
      "- eTimes_13\n",
      "- eDisposition_17\n"
     ]
    }
   ],
   "source": [
    "# Provider Impressions\n",
    "inspect_file_columns(\"../data/raw/FACTPCRPRIMARYIMPRESSION.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRSECONDARYIMPRESSION.txt\")\n",
    "\n",
    "# Symptoms\n",
    "inspect_file_columns(\"../data/raw/FACTPCRPRIMARYSYMPTOM.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRADDITIONALSYMPTOM.txt\")\n",
    "\n",
    "# Cause of Injury and Trauma\n",
    "inspect_file_columns(\"../data/raw/FACTPCRCAUSEOFINJURY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRINJURYRISKFACTOR.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTRAUMACRITERIA.txt\")\n",
    "\n",
    "# Barriers to Care\n",
    "inspect_file_columns(\"../data/raw/FACTPCRBARRIERTOCARE.txt\")\n",
    "\n",
    "# Delay Types\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDISPATCHDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRRESPONSEDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRSCENEDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTRANSPORTDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTURNAROUNDDELAY.txt\")\n",
    "\n",
    "# Destination Details\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDESTINATIONREASON.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDESTINATIONTEAM.txt\")\n",
    "\n",
    "# Vitals\n",
    "inspect_file_columns(\"../data/raw/FACTPCRVITAL.txt\")\n",
    "\n",
    "# Arrest & CPR\n",
    "inspect_file_columns(\"../data/raw/FACTPCRARRESTCPRPROVIDED.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRARRESTRESUSCITATION.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRARRESTRHYTHMDESTINATION.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRARRESTROSC.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRARRESTWITNESS.txt\")\n",
    "\n",
    "# Medication & Procedure\n",
    "inspect_file_columns(\"../data/raw/FACTPCRMEDICATION.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRPROCEDURE.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRPROTOCOL.txt\")\n",
    "\n",
    "# Additional Modes\n",
    "inspect_file_columns(\"../data/raw/FACTPCRADDITIONALRESPONSEMODE.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRADDITIONALTRANSPORTMODE.txt\")\n",
    "\n",
    "# Alcohol/Drug Use Indicator\n",
    "inspect_file_columns(\"../data/raw/FACTPCRALCOHOLDRUGUSEINDICATOR.txt\")\n",
    "\n",
    "# Work Related Exposure\n",
    "inspect_file_columns(\"../data/raw/FACTPCRWORKRELATEDEXPOSURE.txt\")\n",
    "\n",
    "# Patient Groupings\n",
    "inspect_file_columns(\"../data/raw/PCRPATIENTRACEGROUP.txt\")\n",
    "\n",
    "# Procedure/ECG Groupings\n",
    "inspect_file_columns(\"../data/raw/PCRPROCCOMPGROUP.txt\")\n",
    "inspect_file_columns(\"../data/raw/PCRMEDCOMPGROUP.txt\")\n",
    "inspect_file_columns(\"../data/raw/PCRVITALECGGROUP.txt\")\n",
    "inspect_file_columns(\"../data/raw/PCRVITALECGINTERPRETATIONGROUP.txt\")\n",
    "inspect_file_columns(\"../data/raw/PCRVITALGLASGOWQUALIFIERGROUP.txt\")\n",
    "\n",
    "# Core Events Table\n",
    "inspect_file_columns(\"../data/raw/pub_pcrevents_cp25.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67456394-cd26-41fc-af0b-399bd364463b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Primary Impressions: 530it [00:22, 23.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary impressions matched: 231354\n"
     ]
    }
   ],
   "source": [
    "# Define target ICD-10 prefixes\n",
    "target_prefixes = (\"T40\", \"F11\")\n",
    "\n",
    "# Path to primary impressions file\n",
    "primary_path = \"../data/raw/FACTPCRPRIMARYIMPRESSION.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "primary_chunks = []\n",
    "\n",
    "with open(primary_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Processing Primary Impressions\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"eSituation_11\"] = chunk[\"eSituation_11\"].str.strip(\" ~\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        mask = chunk[\"eSituation_11\"].str.startswith(target_prefixes)\n",
    "        filtered = chunk.loc[mask, [\"PcrKey\"]]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            primary_chunks.append(filtered)\n",
    "\n",
    "primary_df = pd.concat(primary_chunks, ignore_index=True)\n",
    "print(\"Primary impressions matched:\", len(primary_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d367fc-6922-4a93-8705-3648ae4a8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Secondary Impressions: 544it [00:24, 22.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secondary impressions matched: 42853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to secondary impressions file\n",
    "secondary_path = \"../data/raw/FACTPCRSECONDARYIMPRESSION.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "secondary_chunks = []\n",
    "\n",
    "with open(secondary_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Processing Secondary Impressions\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"eSituation_12\"] = chunk[\"eSituation_12\"].str.strip(\" ~\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        mask = chunk[\"eSituation_12\"].str.startswith(target_prefixes)\n",
    "        filtered = chunk.loc[mask, [\"PcrKey\"]]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            secondary_chunks.append(filtered)\n",
    "\n",
    "secondary_df = pd.concat(secondary_chunks, ignore_index=True)\n",
    "print(\"Secondary impressions matched:\", len(secondary_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5698a695-cf23-40a5-8e95-f78d438841b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique opioid-related cases: 271206\n"
     ]
    }
   ],
   "source": [
    "# Combine and deduplicate primary and secondary impression matches\n",
    "opioid_cases = pd.concat([primary_df, secondary_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "# Optionally, turn it into a set for fast lookups\n",
    "opioid_pcr_keys = set(opioid_cases[\"PcrKey\"])\n",
    "print(\"Total unique opioid-related cases:\", len(opioid_pcr_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8ab3cf-ba62-48ad-b502-cc28fef66b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering pub_pcrevents_cp25.txt: 542it [05:35,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered events records: 271206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load core events table\n",
    "events_path = \"../data/raw/pub_pcrevents_cp25.txt\"\n",
    "events_chunks = []\n",
    "\n",
    "with open(events_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Filtering pub_pcrevents_cp25.txt\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "        if not filtered.empty:\n",
    "            events_chunks.append(filtered)\n",
    "\n",
    "events_df = pd.concat(events_chunks, ignore_index=True)\n",
    "print(\"Filtered events records:\", len(events_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "560eeb85-996c-4e9d-ad35-51a3155a80f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Primary Symptom: 534it [00:49, 10.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged primary symptom. New shape: (271206, 48)\n"
     ]
    }
   ],
   "source": [
    "# Load and aggregate eSituation_09 (Primary Symptom)\n",
    "symptom_path = \"../data/raw/FACTPCRPRIMARYSYMPTOM.txt\"\n",
    "symptom_chunks = []\n",
    "\n",
    "with open(symptom_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Processing Primary Symptom\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        chunk[\"eSituation_09\"] = chunk[\"eSituation_09\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "        if not filtered.empty:\n",
    "            symptom_chunks.append(filtered[[\"PcrKey\", \"eSituation_09\"]])\n",
    "\n",
    "symptom_df = pd.concat(symptom_chunks, ignore_index=True)\n",
    "\n",
    "# Merge into events_df (one-to-one)\n",
    "events_df = events_df.merge(symptom_df, on=\"PcrKey\", how=\"left\")\n",
    "print(\"Merged primary symptom. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9dc00fd-81aa-4309-8fb2-43518e603bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Associated Symptoms: 580it [00:54, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged associated symptoms. New shape: (271206, 50)\n"
     ]
    }
   ],
   "source": [
    "# Load and aggregate eSituation_10 (Other Associated Symptoms)\n",
    "assoc_path = \"../data/raw/FACTPCRADDITIONALSYMPTOM.txt\"\n",
    "assoc_chunks = []\n",
    "\n",
    "with open(assoc_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Processing Associated Symptoms\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        chunk[\"eSituation_10\"] = chunk[\"eSituation_10\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "        if not filtered.empty:\n",
    "            assoc_chunks.append(filtered[[\"PcrKey\", \"eSituation_10\"]])\n",
    "\n",
    "assoc_df = pd.concat(assoc_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate: count and nunique per PcrKey\n",
    "assoc_agg = (\n",
    "    assoc_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        assoc_symptom_count=(\"eSituation_10\", \"count\"),\n",
    "        assoc_symptom_unique=(\"eSituation_10\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge\n",
    "events_df = events_df.merge(assoc_agg, on=\"PcrKey\", how=\"left\")\n",
    "print(\"Merged associated symptoms. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d332e356-700b-493b-94e7-04ebaf646b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reloading Primary Impressions: 530it [00:48, 10.99it/s]\n",
      "Reloading Secondary Impressions: 544it [00:49, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged primary impression. New shape: (271206, 51)\n",
      "Merged secondary impression. New shape: (281813, 52)\n"
     ]
    }
   ],
   "source": [
    "# Reload and merge full Primary and Secondary Impressions\n",
    "\n",
    "primary_chunks = []\n",
    "secondary_chunks = []\n",
    "\n",
    "# Reload Primary Impressions\n",
    "with open(\"../data/raw/FACTPCRPRIMARYIMPRESSION.txt\", \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Reloading Primary Impressions\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        chunk[\"eSituation_11\"] = chunk[\"eSituation_11\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "        if not filtered.empty:\n",
    "            primary_chunks.append(filtered[[\"PcrKey\", \"eSituation_11\"]])\n",
    "\n",
    "# Reload Secondary Impressions\n",
    "with open(\"../data/raw/FACTPCRSECONDARYIMPRESSION.txt\", \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Reloading Secondary Impressions\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        chunk[\"eSituation_12\"] = chunk[\"eSituation_12\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "        if not filtered.empty:\n",
    "            secondary_chunks.append(filtered[[\"PcrKey\", \"eSituation_12\"]])\n",
    "\n",
    "# Concatenate and merge\n",
    "primary_imp_df = pd.concat(primary_chunks, ignore_index=True)\n",
    "secondary_imp_df = pd.concat(secondary_chunks, ignore_index=True)\n",
    "\n",
    "events_df = events_df.merge(primary_imp_df, on=\"PcrKey\", how=\"left\")\n",
    "print(\"Merged primary impression. New shape:\", events_df.shape)\n",
    "\n",
    "events_df = events_df.merge(secondary_imp_df, on=\"PcrKey\", how=\"left\")\n",
    "print(\"Merged secondary impression. New shape:\", events_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e977938-3ff3-4699-a96d-512892d42974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eInjury_01 (Cause of Injury): 545it [00:20, 26.47it/s]\n",
      "Loading eInjury_04 (Risk Factor): 543it [00:20, 26.34it/s]\n",
      "Loading eInjury_03 (Trauma Criteria): 543it [00:20, 26.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injury-related features merged. New shape: (281813, 55)\n"
     ]
    }
   ],
   "source": [
    "# Loading Injury Data\n",
    "\n",
    "# Paths to source files\n",
    "cause_path = \"../data/raw/FACTPCRCAUSEOFINJURY.txt\"\n",
    "risk_path = \"../data/raw/FACTPCRINJURYRISKFACTOR.txt\"\n",
    "trauma_path = \"../data/raw/FACTPCRTRAUMACRITERIA.txt\"\n",
    "\n",
    "# Initialize lists\n",
    "cause_chunks = []\n",
    "risk_chunks = []\n",
    "trauma_chunks = []\n",
    "\n",
    "# Load Cause of Injury (eInjury_01)\n",
    "with open(cause_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eInjury_01 (Cause of Injury)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eInjury_01\"] = chunk[\"eInjury_01\"].str.strip(\" ~'\")\n",
    "        cause_chunks.append(chunk[[\"PcrKey\", \"eInjury_01\"]])\n",
    "\n",
    "cause_df = pd.concat(cause_chunks, ignore_index=True)\n",
    "\n",
    "# Load Injury Risk Factor (eInjury_04)\n",
    "with open(risk_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eInjury_04 (Risk Factor)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eInjury_04\"] = chunk[\"eInjury_04\"].str.strip(\" ~'\")\n",
    "        risk_chunks.append(chunk[[\"PcrKey\", \"eInjury_04\"]])\n",
    "\n",
    "risk_df = pd.concat(risk_chunks, ignore_index=True)\n",
    "\n",
    "# Load Trauma Criteria (eInjury_03)\n",
    "with open(trauma_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eInjury_03 (Trauma Criteria)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eInjury_03\"] = chunk[\"eInjury_03\"].str.strip(\" ~'\")\n",
    "        trauma_chunks.append(chunk[[\"PcrKey\", \"eInjury_03\"]])\n",
    "\n",
    "trauma_df = pd.concat(trauma_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate each to 1 row per PcrKey\n",
    "cause_agg = cause_df.groupby(\"PcrKey\").agg(\n",
    "    injury_cause_count=(\"eInjury_01\", \"nunique\")\n",
    ").reset_index()\n",
    "\n",
    "risk_agg = (\n",
    "    risk_df.dropna(subset=[\"eInjury_04\"])\n",
    "    .groupby(\"PcrKey\")\n",
    "    .size()\n",
    "    .reset_index(name=\"injury_risk_flag\")\n",
    ")\n",
    "risk_agg[\"injury_risk_flag\"] = 1  # presence = 1\n",
    "\n",
    "trauma_agg = trauma_df.groupby(\"PcrKey\").agg(\n",
    "    trauma_criteria_count=(\"eInjury_03\", \"nunique\")\n",
    ").reset_index()\n",
    "\n",
    "# Merge into events_df\n",
    "events_df = events_df.merge(cause_agg, on=\"PcrKey\", how=\"left\")\n",
    "events_df = events_df.merge(risk_agg, on=\"PcrKey\", how=\"left\")\n",
    "events_df = events_df.merge(trauma_agg, on=\"PcrKey\", how=\"left\")\n",
    "\n",
    "# Fill missing with 0\n",
    "events_df[\"injury_cause_count\"] = events_df[\"injury_cause_count\"].fillna(0).astype(int)\n",
    "events_df[\"injury_risk_flag\"] = events_df[\"injury_risk_flag\"].fillna(0).astype(int)\n",
    "events_df[\"trauma_criteria_count\"] = events_df[\"trauma_criteria_count\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"Injury-related features merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a0d8baa-e70a-43f0-8710-8b82782a5986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eHistory_01 (Barriers to Care): 550it [00:50, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barriers to care merged. New shape: (281813, 58)\n"
     ]
    }
   ],
   "source": [
    "# Load and process eHistory_01 (Barriers to Patient Care)\n",
    "barrier_path = \"../data/raw/FACTPCRBARRIERTOCARE.txt\"\n",
    "barrier_chunks = []\n",
    "\n",
    "with open(barrier_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eHistory_01 (Barriers to Care)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eHistory_01\"] = chunk[\"eHistory_01\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            barrier_chunks.append(filtered[[\"PcrKey\", \"eHistory_01\"]])\n",
    "\n",
    "barrier_df = pd.concat(barrier_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate\n",
    "barrier_agg = (\n",
    "    barrier_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        barrier_count=(\"eHistory_01\", \"count\"),\n",
    "        barrier_unique=(\"eHistory_01\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "barrier_agg[\"barrier_present\"] = 1\n",
    "\n",
    "# Merge into main dataset\n",
    "events_df = events_df.merge(barrier_agg, on=\"PcrKey\", how=\"left\")\n",
    "\n",
    "# Fill NA with zeros\n",
    "events_df[\"barrier_count\"] = events_df[\"barrier_count\"].fillna(0).astype(int)\n",
    "events_df[\"barrier_unique\"] = events_df[\"barrier_unique\"].fillna(0).astype(int)\n",
    "events_df[\"barrier_present\"] = events_df[\"barrier_present\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"Barriers to care merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f0e6e5-d506-4917-b09e-e9fb8649afaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eResponse_08 from ../data/raw/FACTPCRDISPATCHDELAY.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eResponse_08: 543it [00:51, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eResponse_09 from ../data/raw/FACTPCRRESPONSEDELAY.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eResponse_09: 545it [00:51, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eResponse_10 from ../data/raw/FACTPCRSCENEDELAY.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eResponse_10: 546it [00:56,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eResponse_11 from ../data/raw/FACTPCRTRANSPORTDELAY.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eResponse_11: 544it [00:55,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eResponse_12 from ../data/raw/FACTPCRTURNAROUNDDELAY.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eResponse_12: 552it [00:55,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All delay types merged. New shape: (281813, 73)\n"
     ]
    }
   ],
   "source": [
    "# Delay files and their corresponding column names\n",
    "delay_sources = {\n",
    "    \"../data/raw/FACTPCRDISPATCHDELAY.txt\": \"eResponse_08\",\n",
    "    \"../data/raw/FACTPCRRESPONSEDELAY.txt\": \"eResponse_09\",\n",
    "    \"../data/raw/FACTPCRSCENEDELAY.txt\": \"eResponse_10\",\n",
    "    \"../data/raw/FACTPCRTRANSPORTDELAY.txt\": \"eResponse_11\",\n",
    "    \"../data/raw/FACTPCRTURNAROUNDDELAY.txt\": \"eResponse_12\",\n",
    "}\n",
    "\n",
    "# Loop through each and join delay features\n",
    "for file_path, col in delay_sources.items():\n",
    "    print(f\"Processing {col} from {file_path}\")\n",
    "    delay_chunks = []\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for chunk in tqdm(\n",
    "            pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "            desc=f\"Loading {col}\"\n",
    "        ):\n",
    "            chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "            chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "            chunk[col] = chunk[col].str.strip(\" ~'\")\n",
    "\n",
    "            filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "            if not filtered.empty:\n",
    "                delay_chunks.append(filtered[[\"PcrKey\", col]])\n",
    "\n",
    "    delay_df = pd.concat(delay_chunks, ignore_index=True)\n",
    "\n",
    "    # Aggregate delay types per PcrKey\n",
    "    delay_agg = (\n",
    "        delay_df.groupby(\"PcrKey\")[col]\n",
    "        .agg([\n",
    "            (\"{}_count\".format(col), \"count\"),\n",
    "            (\"{}_unique\".format(col), \"nunique\")\n",
    "        ])\n",
    "        .reset_index()\n",
    "    )\n",
    "    delay_agg[\"{}_present\".format(col)] = 1\n",
    "\n",
    "    # Merge into main events_df\n",
    "    events_df = events_df.merge(delay_agg, on=\"PcrKey\", how=\"left\")\n",
    "\n",
    "    # Fill missing values\n",
    "    events_df[\"{}_count\".format(col)] = events_df[\"{}_count\".format(col)].fillna(0).astype(int)\n",
    "    events_df[\"{}_unique\".format(col)] = events_df[\"{}_unique\".format(col)].fillna(0).astype(int)\n",
    "    events_df[\"{}_present\".format(col)] = events_df[\"{}_present\".format(col)].fillna(0).astype(int)\n",
    "\n",
    "print(\"All delay types merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a467c8ac-5103-4405-b6d3-2f399fbcf17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eDisposition_20 from ../data/raw/FACTPCRDESTINATIONREASON.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eDisposition_20: 582it [00:41, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eDisposition_24 & eDisposition_25 from ../data/raw/FACTPCRDESTINATIONTEAM.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eDisposition_24 & 25: 543it [00:47, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destination-related features merged. New shape: (297762, 76)\n"
     ]
    }
   ],
   "source": [
    "# Load Destination Reason and Alert Info\n",
    "\n",
    "# File paths\n",
    "dest_reason_path = \"../data/raw/FACTPCRDESTINATIONREASON.txt\"\n",
    "dest_team_path = \"../data/raw/FACTPCRDESTINATIONTEAM.txt\"\n",
    "\n",
    "# Chunks\n",
    "reason_chunks = []\n",
    "team_chunks = []\n",
    "\n",
    "# eDisposition_20 - Reason for choosing destination\n",
    "print(f\"Processing eDisposition_20 from {dest_reason_path}\")\n",
    "with open(dest_reason_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eDisposition_20\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eDisposition_20\"] = chunk[\"eDisposition_20\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "        if not filtered.empty:\n",
    "            reason_chunks.append(filtered[[\"PcrKey\", \"eDisposition_20\"]])\n",
    "\n",
    "reason_df = pd.concat(reason_chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "# eDisposition_24 and eDisposition_25 - Alert type and timestamp\n",
    "print(f\"Processing eDisposition_24 & eDisposition_25 from {dest_team_path}\")\n",
    "with open(dest_team_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eDisposition_24 & 25\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eDisposition_24\"] = chunk[\"eDisposition_24\"].str.strip(\" ~'\")\n",
    "        chunk[\"eDisposition_25\"] = chunk[\"eDisposition_25\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "        if not filtered.empty:\n",
    "            team_chunks.append(filtered[[\"PcrKey\", \"eDisposition_24\", \"eDisposition_25\"]])\n",
    "\n",
    "team_df = pd.concat(team_chunks, ignore_index=True)\n",
    "\n",
    "# Merge into events_df\n",
    "events_df = events_df.merge(reason_df, on=\"PcrKey\", how=\"left\")\n",
    "events_df = events_df.merge(team_df, on=\"PcrKey\", how=\"left\")\n",
    "\n",
    "print(\"Destination-related features merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05e8c89f-2bce-4a9b-a789-f98e17e208c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Vitals: 1648it [05:34,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vitals merged. New shape: (297762, 82)\n"
     ]
    }
   ],
   "source": [
    "# Load and aggregate vitals from FACTPCRVITAL.txt\n",
    "\n",
    "vitals_path = \"../data/raw/FACTPCRVITAL.txt\"\n",
    "vitals_chunks = []\n",
    "\n",
    "# Vital fields of interest and new names\n",
    "vital_fields = {\n",
    "    \"eVitals_10\": \"heart_rate\",\n",
    "    \"eVitals_14\": \"resp_rate\",\n",
    "    \"eVitals_06\": \"systolic_bp\",\n",
    "    \"eVitals_12\": \"spo2\",\n",
    "    \"eVitals_18\": \"bgl\",\n",
    "    \"eVitals_16\": \"etco2\"\n",
    "}\n",
    "\n",
    "with open(vitals_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Vitals\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk = chunk[[\"PcrKey\"] + list(vital_fields.keys())]\n",
    "        chunk = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        vitals_chunks.append(chunk)\n",
    "\n",
    "vitals_df = pd.concat(vitals_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate: take the first non-null value per PcrKey per vital\n",
    "agg_dict = {col: \"first\" for col in vital_fields.keys()}\n",
    "vitals_agg = (\n",
    "    vitals_df.groupby(\"PcrKey\")\n",
    "    .agg(agg_dict)\n",
    "    .rename(columns=vital_fields)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge into main dataset\n",
    "events_df = events_df.merge(vitals_agg, on=\"PcrKey\", how=\"left\")\n",
    "print(\"Vitals merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c7dc0a-db89-4c29-9950-354a4d1e9b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Medications: 628it [02:02,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medications merged. New shape: (297762, 85)\n"
     ]
    }
   ],
   "source": [
    "# Load and process medication administration data\n",
    "\n",
    "med_path = \"../data/raw/FACTPCRMEDICATION.txt\"\n",
    "med_chunks = []\n",
    "\n",
    "with open(med_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Medications\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eMedications_03\"] = chunk[\"eMedications_03\"].str.strip(\" ~'\")\n",
    "        chunk[\"eMedications_03Descr\"] = chunk[\"eMedications_03Descr\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            med_chunks.append(filtered[[\"PcrKey\", \"eMedications_03\", \"eMedications_03Descr\"]])\n",
    "\n",
    "med_df = pd.concat(med_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate\n",
    "med_agg = (\n",
    "    med_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        any_med_administered=(\"eMedications_03\", lambda x: 1),\n",
    "        med_count=(\"eMedications_03\", \"nunique\"),\n",
    "        naloxone_administered=(\"eMedications_03Descr\", lambda x: int(x.str.contains(\"naloxone|narcan\", case=False, na=False).any()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge\n",
    "events_df = events_df.merge(med_agg, on=\"PcrKey\", how=\"left\")\n",
    "events_df[[\"any_med_administered\", \"med_count\", \"naloxone_administered\"]] = events_df[[\"any_med_administered\", \"med_count\", \"naloxone_administered\"]].fillna(0).astype(int)\n",
    "\n",
    "print(\"Medications merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6140ae72-3ef4-4282-987c-a6bd689c503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Procedures: 934it [02:03,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedures merged. New shape: (297762, 87)\n"
     ]
    }
   ],
   "source": [
    "# Load and process procedure data\n",
    "\n",
    "proc_path = \"../data/raw/FACTPCRPROCEDURE.txt\"\n",
    "proc_chunks = []\n",
    "\n",
    "with open(proc_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Procedures\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eProcedures_03\"] = chunk[\"eProcedures_03\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            proc_chunks.append(filtered[[\"PcrKey\", \"eProcedures_03\"]])\n",
    "\n",
    "proc_df = pd.concat(proc_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate\n",
    "proc_agg = (\n",
    "    proc_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        any_procedure=(\"eProcedures_03\", lambda x: 1),\n",
    "        proc_count=(\"eProcedures_03\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge\n",
    "events_df = events_df.merge(proc_agg, on=\"PcrKey\", how=\"left\")\n",
    "events_df[[\"any_procedure\", \"proc_count\"]] = events_df[[\"any_procedure\", \"proc_count\"]].fillna(0).astype(int)\n",
    "\n",
    "print(\"Procedures merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d959ab6-2c7a-4f73-bed1-d2686700a565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Alcohol/Drug Use Indicator: 553it [00:40, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol/drug use merged. New shape: (297762, 88)\n"
     ]
    }
   ],
   "source": [
    "# Alcohol/Drug Use Indicator\n",
    "\n",
    "alcohol_path = \"../data/raw/FACTPCRALCOHOLDRUGUSEINDICATOR.txt\"\n",
    "alcohol_chunks = []\n",
    "\n",
    "with open(alcohol_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Alcohol/Drug Use Indicator\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eHistory_17\"] = chunk[\"eHistory_17\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            alcohol_chunks.append(filtered[[\"PcrKey\"]])\n",
    "\n",
    "if alcohol_chunks:\n",
    "    alcohol_df = pd.concat(alcohol_chunks, ignore_index=True).drop_duplicates()\n",
    "    alcohol_df[\"alcohol_drug_use_flag\"] = 1\n",
    "\n",
    "    events_df = events_df.merge(alcohol_df, on=\"PcrKey\", how=\"left\")\n",
    "else:\n",
    "    events_df[\"alcohol_drug_use_flag\"] = 0  # add column if no matches\n",
    "\n",
    "# Always ensure column exists and is filled\n",
    "if \"alcohol_drug_use_flag\" not in events_df.columns:\n",
    "    events_df[\"alcohol_drug_use_flag\"] = 0\n",
    "else:\n",
    "    events_df[\"alcohol_drug_use_flag\"] = events_df[\"alcohol_drug_use_flag\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"Alcohol/drug use merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73e8911b-c8ef-4a71-94df-c88642bac47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Protocols: 556it [00:43, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocols merged. New shape: (297762, 90)\n"
     ]
    }
   ],
   "source": [
    "# Protocols Initiated\n",
    "\n",
    "protocol_path = \"../data/raw/FACTPCRPROTOCOL.txt\"\n",
    "protocol_chunks = []\n",
    "\n",
    "with open(protocol_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Protocols\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eProtocols_01\"] = chunk[\"eProtocols_01\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            protocol_chunks.append(filtered[[\"PcrKey\", \"eProtocols_01\"]])\n",
    "\n",
    "protocol_df = pd.concat(protocol_chunks, ignore_index=True)\n",
    "\n",
    "# Group all protocol codes per PcrKey into comma-separated strings\n",
    "protocol_agg = (\n",
    "    protocol_df.groupby(\"PcrKey\")[\"eProtocols_01\"]\n",
    "    .apply(lambda x: \",\".join(sorted(set(x))))\n",
    "    .reset_index()\n",
    "    .rename(columns={\"eProtocols_01\": \"protocols_used\"})\n",
    ")\n",
    "\n",
    "# Also count how many unique protocols were used\n",
    "protocol_agg[\"protocol_count\"] = protocol_agg[\"protocols_used\"].apply(lambda x: len(x.split(\",\")))\n",
    "\n",
    "events_df = events_df.merge(protocol_agg, on=\"PcrKey\", how=\"left\")\n",
    "events_df[\"protocol_count\"] = events_df[\"protocol_count\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"Protocols merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdfc657c-196d-4701-866a-c21831744ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eArrest_04: 542it [00:36, 14.68it/s]\n",
      "Loading eArrest_03: 546it [00:37, 14.69it/s]\n",
      "Loading eArrest_09: 546it [00:37, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrest-related flags merged. New shape: (297762, 93)\n"
     ]
    }
   ],
   "source": [
    "# Arrest-related Flags (Witnessed, Resuscitation, CPR)\n",
    "\n",
    "def load_arrest_flags(file_path, field, flag_name):\n",
    "    chunks = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for chunk in tqdm(\n",
    "            pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "            desc=f\"Loading {field}\"\n",
    "        ):\n",
    "            chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "            chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "            chunk[field] = chunk[field].str.strip(\" ~'\")\n",
    "            filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "            if not filtered.empty:\n",
    "                chunks.append(filtered[[\"PcrKey\"]])\n",
    "\n",
    "    if chunks:\n",
    "        df = pd.concat(chunks, ignore_index=True).drop_duplicates()\n",
    "        df[flag_name] = 1\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"PcrKey\", flag_name])\n",
    "\n",
    "arrest_flags = []\n",
    "\n",
    "arrest_flags.append(load_arrest_flags(\"../data/raw/FACTPCRARRESTWITNESS.txt\", \"eArrest_04\", \"arrest_witnessed_flag\"))\n",
    "arrest_flags.append(load_arrest_flags(\"../data/raw/FACTPCRARRESTRESUSCITATION.txt\", \"eArrest_03\", \"resuscitation_flag\"))\n",
    "arrest_flags.append(load_arrest_flags(\"../data/raw/FACTPCRARRESTCPRPROVIDED.txt\", \"eArrest_09\", \"cpr_provided_flag\"))\n",
    "\n",
    "# Merge each into events_df\n",
    "for flag_df in arrest_flags:\n",
    "    events_df = events_df.merge(flag_df, on=\"PcrKey\", how=\"left\")\n",
    "\n",
    "# Fill missing with 0\n",
    "for col in [\"arrest_witnessed_flag\", \"resuscitation_flag\", \"cpr_provided_flag\"]:\n",
    "    events_df[col] = events_df[col].fillna(0).astype(int)\n",
    "\n",
    "print(\"Arrest-related flags merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53f8d511-8e6f-45b1-84d6-f2517dc7360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eArrest_12 (ROSC): 543it [00:39, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSC merged. New shape: (297762, 94)\n"
     ]
    }
   ],
   "source": [
    "# ROSC Flag (Return of Spontaneous Circulation)\n",
    "\n",
    "roscs = []\n",
    "rosc_path = \"../data/raw/FACTPCRARRESTROSC.txt\"\n",
    "\n",
    "with open(rosc_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eArrest_12 (ROSC)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eArrest_12\"] = chunk[\"eArrest_12\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            roscs.append(filtered[[\"PcrKey\"]])\n",
    "\n",
    "if roscs:\n",
    "    rosc_df = pd.concat(roscs, ignore_index=True).drop_duplicates()\n",
    "    rosc_df[\"rosc_flag\"] = 1\n",
    "    events_df = events_df.merge(rosc_df, on=\"PcrKey\", how=\"left\")\n",
    "    events_df[\"rosc_flag\"] = events_df[\"rosc_flag\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"ROSC merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92d9cdf8-bec5-4192-a216-18a35ada2761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eArrest_17 (Destination Rhythm): 542it [00:37, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destination arrest rhythm merged. New shape: (297762, 95)\n"
     ]
    }
   ],
   "source": [
    "# Arrest Rhythm at Destination\n",
    "\n",
    "rhythm_chunks = []\n",
    "rhythm_path = \"../data/raw/FACTPCRARRESTRHYTHMDESTINATION.txt\"\n",
    "\n",
    "with open(rhythm_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eArrest_17 (Destination Rhythm)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eArrest_17\"] = chunk[\"eArrest_17\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            rhythm_chunks.append(filtered[[\"PcrKey\", \"eArrest_17\"]])\n",
    "\n",
    "if rhythm_chunks:\n",
    "    rhythm_df = pd.concat(rhythm_chunks, ignore_index=True)\n",
    "    rhythm_agg = (\n",
    "        rhythm_df.groupby(\"PcrKey\")\n",
    "        .agg(dest_rhythm_count=(\"eArrest_17\", \"nunique\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    events_df = events_df.merge(rhythm_agg, on=\"PcrKey\", how=\"left\")\n",
    "    events_df[\"dest_rhythm_count\"] = events_df[\"dest_rhythm_count\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"Destination arrest rhythm merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb154309-1a19-4965-9b88-d16d6998ac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ePatient_14 (Race Group): 548it [00:46, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient race info merged. New shape: (297762, 96)\n"
     ]
    }
   ],
   "source": [
    "# Patient Race Group\n",
    "\n",
    "race_chunks = []\n",
    "race_path = \"../data/raw/PCRPATIENTRACEGROUP.txt\"\n",
    "\n",
    "with open(race_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading ePatient_14 (Race Group)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"ePatient_14\"] = chunk[\"ePatient_14\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            race_chunks.append(filtered[[\"PcrKey\", \"ePatient_14\"]])\n",
    "\n",
    "if race_chunks:\n",
    "    race_df = pd.concat(race_chunks, ignore_index=True)\n",
    "    race_agg = (\n",
    "        race_df.groupby(\"PcrKey\")\n",
    "        .agg(race_count=(\"ePatient_14\", \"nunique\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    events_df = events_df.merge(race_agg, on=\"PcrKey\", how=\"left\")\n",
    "    events_df[\"race_count\"] = events_df[\"race_count\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"Patient race info merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d8179f3-799b-4125-9921-dbdf18766474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eResponse_24 (Response Mode): 612it [00:41, 14.59it/s]\n",
      "Loading eDisposition_18 (Transport Mode): 578it [00:40, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response and transport mode merged. New shape: (297762, 98)\n"
     ]
    }
   ],
   "source": [
    "# Response Mode and Transport Mode\n",
    "\n",
    "response_path = \"../data/raw/FACTPCRADDITIONALRESPONSEMODE.txt\"\n",
    "transport_path = \"../data/raw/FACTPCRADDITIONALTRANSPORTMODE.txt\"\n",
    "\n",
    "# Load response mode (eResponse_24)\n",
    "resp_chunks = []\n",
    "with open(response_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eResponse_24 (Response Mode)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eResponse_24\"] = chunk[\"eResponse_24\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            resp_chunks.append(filtered[[\"PcrKey\", \"eResponse_24\"]])\n",
    "\n",
    "if resp_chunks:\n",
    "    resp_df = pd.concat(resp_chunks, ignore_index=True).drop_duplicates(\"PcrKey\")\n",
    "    events_df = events_df.merge(resp_df, on=\"PcrKey\", how=\"left\")\n",
    "\n",
    "# Load transport mode (eDisposition_18)\n",
    "trans_chunks = []\n",
    "with open(transport_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eDisposition_18 (Transport Mode)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eDisposition_18\"] = chunk[\"eDisposition_18\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            trans_chunks.append(filtered[[\"PcrKey\", \"eDisposition_18\"]])\n",
    "\n",
    "if trans_chunks:\n",
    "    trans_df = pd.concat(trans_chunks, ignore_index=True).drop_duplicates(\"PcrKey\")\n",
    "    events_df = events_df.merge(trans_df, on=\"PcrKey\", how=\"left\")\n",
    "\n",
    "print(\"Response and transport mode merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bce32d63-1e99-469c-8b6f-726a1630407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment if running this cell for the first time ##\n",
    "\n",
    "# # Build and save VitalKey → PcrKey mapping\n",
    "# vitals_map_chunks = []\n",
    "\n",
    "# with open(\"../data/raw/FACTPCRVITAL.txt\", \"r\") as f:\n",
    "#     for chunk in tqdm(\n",
    "#         pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "#         desc=\"Building VitalKey → PcrKey map\"\n",
    "#     ):\n",
    "#         chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "#         if \"PcrVitalKey\" in chunk.columns and \"PcrKey\" in chunk.columns:\n",
    "#             chunk[\"PcrVitalKey\"] = chunk[\"PcrVitalKey\"].str.strip(\" ~'\")\n",
    "#             chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "#             vitals_map_chunks.append(chunk[[\"PcrVitalKey\", \"PcrKey\"]])\n",
    "\n",
    "# vitals_map_df = pd.concat(vitals_map_chunks, ignore_index=True).dropna()\n",
    "\n",
    "# # Optional: drop duplicates just in case\n",
    "# vitals_map_df = vitals_map_df.drop_duplicates()\n",
    "\n",
    "# # Save to CSV for reuse\n",
    "# vitals_map_df.to_csv(\"../data/interim/vitalkey_map.csv\", index=False)\n",
    "# print(\"Saved VitalKey → PcrKey mapping to vitalkey_map.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4843db4c-d7e7-4023-a095-e4772bdf49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Glasgow Qualifier\n",
    "\n",
    "# # Load prebuilt VitalKey → PcrKey mapping\n",
    "# vital_map_df = pd.read_csv(\"../data/interim/vitalkey_map.csv\", dtype=str)\n",
    "# vital_map_df = vital_map_df.dropna().drop_duplicates()\n",
    "\n",
    "# # Load Glasgow Qualifier data\n",
    "# glasgow_path = \"../data/raw/PCRVITALGLASGOWQUALIFIERGROUP.txt\"\n",
    "# glasgow_chunks = []\n",
    "\n",
    "# with open(glasgow_path, \"r\") as f:\n",
    "#     for chunk in tqdm(\n",
    "#         pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "#         desc=\"Loading eVitals_22 (Glasgow Qualifier)\"\n",
    "#     ):\n",
    "#         chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "#         chunk[\"VitalKey\"] = chunk[\"VitalKey\"].str.strip(\" ~'\")\n",
    "#         chunk[\"eVitals_22\"] = chunk[\"eVitals_22\"].str.strip(\" ~'\")\n",
    "#         filtered = chunk[chunk[\"VitalKey\"].notna()]\n",
    "#         if not filtered.empty:\n",
    "#             glasgow_chunks.append(filtered[[\"VitalKey\", \"eVitals_22\"]])\n",
    "\n",
    "# # Concatenate\n",
    "# if glasgow_chunks:\n",
    "#     glasgow_df = pd.concat(glasgow_chunks, ignore_index=True)\n",
    "\n",
    "#     # Merge VitalKey → PcrKey\n",
    "#     glasgow_df = glasgow_df.merge(\n",
    "#         vital_map_df,\n",
    "#         left_on=\"VitalKey\",\n",
    "#         right_on=\"PcrVitalKey\",\n",
    "#         how=\"left\"\n",
    "#     )\n",
    "\n",
    "#     # Aggregate count of unique GCS scores per PcrKey\n",
    "#     glasgow_agg = (\n",
    "#         glasgow_df.dropna(subset=[\"PcrKey\"])\n",
    "#         .groupby(\"PcrKey\")\n",
    "#         .agg(glasgow_score_count=(\"eVitals_22\", \"nunique\"))\n",
    "#         .reset_index()\n",
    "#     )\n",
    "\n",
    "#     events_df = events_df.merge(glasgow_agg, on=\"PcrKey\", how=\"left\")\n",
    "#     events_df[\"glasgow_score_count\"] = events_df[\"glasgow_score_count\"].fillna(0).astype(int)\n",
    "\n",
    "# print(\"Glasgow score info merged. New shape:\", events_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d27af5cd-c558-468d-abcb-2581c34113c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ECG Interpretation\n",
    "\n",
    "# # Load pre-built mapping\n",
    "# vitals_map_df = pd.read_csv(\"../data/interim/vitalkey_map.csv\", dtype=str)\n",
    "# vitals_map_df[\"PcrVitalKey\"] = vitals_map_df[\"PcrVitalKey\"].str.strip(\" ~'\")\n",
    "# vitals_map_df[\"PcrKey\"] = vitals_map_df[\"PcrKey\"].str.strip(\" ~'\")\n",
    "\n",
    "# # Load ECG interpretation data\n",
    "# ecg_path = \"../data/raw/PCRVITALECGINTERPRETATIONGROUP.txt\"\n",
    "# ecg_chunks = []\n",
    "\n",
    "# with open(ecg_path, \"r\") as f:\n",
    "#     for chunk in tqdm(\n",
    "#         pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "#         desc=\"Loading eVitals_05 (ECG Interpretation)\"\n",
    "#     ):\n",
    "#         chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "#         chunk[\"VitalKey\"] = chunk[\"VitalKey\"].str.strip(\" ~'\")\n",
    "#         chunk[\"eVitals_05\"] = chunk[\"eVitals_05\"].str.strip(\" ~'\")\n",
    "#         filtered = chunk[chunk[\"VitalKey\"].notna()]\n",
    "#         if not filtered.empty:\n",
    "#             ecg_chunks.append(filtered[[\"VitalKey\", \"eVitals_05\"]])\n",
    "\n",
    "# if ecg_chunks:\n",
    "#     ecg_df = pd.concat(ecg_chunks, ignore_index=True)\n",
    "    \n",
    "#     # Merge with saved map\n",
    "#     ecg_df = ecg_df.merge(\n",
    "#         vitals_map_df,\n",
    "#         left_on=\"VitalKey\",\n",
    "#         right_on=\"PcrVitalKey\",\n",
    "#         how=\"left\"\n",
    "#     )\n",
    "\n",
    "#     # Aggregate by PcrKey\n",
    "#     ecg_agg = (\n",
    "#         ecg_df.dropna(subset=[\"PcrKey\"])\n",
    "#         .groupby(\"PcrKey\")\n",
    "#         .agg(ecg_interpretation_count=(\"eVitals_05\", \"nunique\"))\n",
    "#         .reset_index()\n",
    "#     )\n",
    "\n",
    "#     events_df = events_df.merge(ecg_agg, on=\"PcrKey\", how=\"left\")\n",
    "#     events_df[\"ecg_interpretation_count\"] = events_df[\"ecg_interpretation_count\"].fillna(0).astype(int)\n",
    "\n",
    "# print(\"ECG interpretation merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11212fdd-60c0-4864-8b5e-dfcab8805d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Medication Complications\n",
    "\n",
    "# medcomp_path = \"../data/raw/PCRMEDCOMPGROUP.txt\"\n",
    "# medcomp_chunks = []\n",
    "\n",
    "# with open(medcomp_path, \"r\") as f:\n",
    "#     for chunk in tqdm(\n",
    "#         pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "#         desc=\"Loading eMedications_08 (Medication Complications)\"\n",
    "#     ):\n",
    "#         chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "#         chunk[\"MedicationKey\"] = chunk[\"MedicationKey\"].str.strip(\" ~'\")\n",
    "#         chunk[\"eMedications_08\"] = chunk[\"eMedications_08\"].str.strip(\" ~'\")\n",
    "#         filtered = chunk[chunk[\"MedicationKey\"].notna()]\n",
    "\n",
    "#         if not filtered.empty:\n",
    "#             medcomp_chunks.append(filtered[[\"MedicationKey\", \"eMedications_08\"]])\n",
    "\n",
    "# if medcomp_chunks:\n",
    "#     medcomp_df = pd.concat(medcomp_chunks, ignore_index=True)\n",
    "\n",
    "#     # Link to PcrKey using FACTPCRMEDICATION\n",
    "#     med_link = pd.read_csv(\"../data/raw/FACTPCRMEDICATION.txt\", delimiter=\"|\", dtype=str)\n",
    "#     med_link[\"PcrKey\"] = med_link[\"PcrKey\"].str.strip(\" ~'\")\n",
    "#     med_link[\"PcrMedicationKey\"] = med_link[\"PcrMedicationKey\"].str.strip(\" ~'\")\n",
    "\n",
    "#     medcomp_df = medcomp_df.merge(\n",
    "#         med_link[[\"PcrMedicationKey\", \"PcrKey\"]],\n",
    "#         left_on=\"MedicationKey\",\n",
    "#         right_on=\"PcrMedicationKey\",\n",
    "#         how=\"left\"\n",
    "#     )\n",
    "\n",
    "#     medcomp_agg = (\n",
    "#         medcomp_df.groupby(\"PcrKey\")\n",
    "#         .agg(med_complication_count=(\"eMedications_08\", \"nunique\"))\n",
    "#         .reset_index()\n",
    "#     )\n",
    "\n",
    "#     events_df = events_df.merge(medcomp_agg, on=\"PcrKey\", how=\"left\")\n",
    "#     events_df[\"med_complication_count\"] = events_df[\"med_complication_count\"].fillna(0).astype(int)\n",
    "\n",
    "# print(\"Medication complications merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e7ce144-41ed-44f7-8b52-bf88cead0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Procedure Complications\n",
    "\n",
    "# proccomp_path = \"../data/raw/PCRPROCCOMPGROUP.txt\"\n",
    "# proccomp_chunks = []\n",
    "\n",
    "# with open(proccomp_path, \"r\") as f:\n",
    "#     for chunk in tqdm(\n",
    "#         pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "#         desc=\"Loading eProcedures_07 (Procedure Complications)\"\n",
    "#     ):\n",
    "#         chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "#         chunk[\"ProcedureKey\"] = chunk[\"ProcedureKey\"].str.strip(\" ~'\")\n",
    "#         chunk[\"eProcedures_07\"] = chunk[\"eProcedures_07\"].str.strip(\" ~'\")\n",
    "#         filtered = chunk[chunk[\"ProcedureKey\"].notna()]\n",
    "\n",
    "#         if not filtered.empty:\n",
    "#             proccomp_chunks.append(filtered[[\"ProcedureKey\", \"eProcedures_07\"]])\n",
    "\n",
    "# if proccomp_chunks:\n",
    "#     proccomp_df = pd.concat(proccomp_chunks, ignore_index=True)\n",
    "\n",
    "#     # Link to PcrKey using FACTPCRPROCEDURE\n",
    "#     proc_link = pd.read_csv(\"../data/raw/FACTPCRPROCEDURE.txt\", delimiter=\"|\", dtype=str)\n",
    "#     proc_link[\"PcrKey\"] = proc_link[\"PcrKey\"].str.strip(\" ~'\")\n",
    "#     proc_link[\"PcrProcedureKey\"] = proc_link[\"PcrProcedureKey\"].str.strip(\" ~'\")\n",
    "\n",
    "#     proccomp_df = proccomp_df.merge(\n",
    "#         proc_link[[\"PcrProcedureKey\", \"PcrKey\"]],\n",
    "#         left_on=\"ProcedureKey\",\n",
    "#         right_on=\"PcrProcedureKey\",\n",
    "#         how=\"left\"\n",
    "#     )\n",
    "\n",
    "#     proccomp_agg = (\n",
    "#         proccomp_df.groupby(\"PcrKey\")\n",
    "#         .agg(proc_complication_count=(\"eProcedures_07\", \"nunique\"))\n",
    "#         .reset_index()\n",
    "#     )\n",
    "\n",
    "#     events_df = events_df.merge(proccomp_agg, on=\"PcrKey\", how=\"left\")\n",
    "#     events_df[\"proc_complication_count\"] = events_df[\"proc_complication_count\"].fillna(0).astype(int)\n",
    "\n",
    "# print(\"Procedure complications merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "218f6d09-2b2a-450b-9db2-568fb28a5017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (297762, 98)\n",
      "\n",
      "Missing values per column:\n",
      "eSituation_11           5996\n",
      "eSituation_09           5429\n",
      "eSituation_12           4196\n",
      "assoc_symptom_count     3715\n",
      "assoc_symptom_unique    3715\n",
      "etco2                    145\n",
      "bgl                      145\n",
      "heart_rate               145\n",
      "resp_rate                145\n",
      "systolic_bp              145\n",
      "spo2                     145\n",
      "dtype: int64\n",
      "\n",
      "Unique values per column (non-null):\n",
      "PcrKey                  271206\n",
      "eTimes_05               264238\n",
      "eTimes_06               263980\n",
      "eTimes_03               261992\n",
      "eTimes_13               258661\n",
      "                         ...  \n",
      "any_procedure                1\n",
      "eResponse_09_present         1\n",
      "eResponse_12_present         1\n",
      "eResponse_11_present         1\n",
      "barrier_present              1\n",
      "Length: 98, dtype: int64\n",
      "\n",
      "Value counts for selected columns:\n",
      "\n",
      "--- eOutcome_01 ---\n",
      "eOutcome_01\n",
      "~7701003   ~    247485\n",
      "~7701001   ~     44600\n",
      "~01        ~      2240\n",
      "~30        ~      1450\n",
      "~09        ~       955\n",
      "~07        ~       315\n",
      "~02        ~       288\n",
      "~65        ~       166\n",
      "~21        ~        88\n",
      "~70        ~        49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- eOutcome_02 ---\n",
      "eOutcome_02\n",
      "~7701003   ~    231392\n",
      "~7701001   ~     63256\n",
      "~30        ~      1311\n",
      "~01        ~      1085\n",
      "~07        ~       184\n",
      "~02        ~       128\n",
      "~65        ~       122\n",
      "~06        ~        60\n",
      "~20        ~        55\n",
      "~70        ~        44\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- eSituation_11 ---\n",
      "eSituation_11\n",
      "T40.2X4     59794\n",
      "F11         58826\n",
      "F11.9       42575\n",
      "T40.1X4     21306\n",
      "R41.82       9000\n",
      "F11.10       6472\n",
      "NaN          5996\n",
      "T40.1X1A     5034\n",
      "T40.4X4      4745\n",
      "T40.2X1A     4626\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- eSituation_12 ---\n",
      "eSituation_12\n",
      "7701003    142815\n",
      "7701001     31929\n",
      "F11         13594\n",
      "R41.82      10519\n",
      "F11.9        8435\n",
      "T40.2X4      8294\n",
      "Z00.00       6075\n",
      "NaN          4196\n",
      "R53.1        2593\n",
      "R40.20       2487\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- injury_cause_count ---\n",
      "injury_cause_count\n",
      "1    297074\n",
      "2       605\n",
      "3        36\n",
      "5        34\n",
      "4        13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- injury_risk_flag ---\n",
      "injury_risk_flag\n",
      "1    297637\n",
      "0       125\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- trauma_criteria_count ---\n",
      "trauma_criteria_count\n",
      "1    297456\n",
      "2       207\n",
      "3        81\n",
      "0        15\n",
      "4         2\n",
      "5         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- alcohol_drug_use_flag ---\n",
      "alcohol_drug_use_flag\n",
      "1    297762\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Final DataFrame Inspection\n",
    "\n",
    "print(\"Final shape:\", events_df.shape)\n",
    "\n",
    "# Count of missing values per column\n",
    "null_counts = events_df.isnull().sum().sort_values(ascending=False)\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(null_counts[null_counts > 0])\n",
    "\n",
    "# Count of unique values per column (helps spot single-value or ID-like fields)\n",
    "print(\"\\nUnique values per column (non-null):\")\n",
    "print(events_df.nunique().sort_values(ascending=False))\n",
    "\n",
    "# Quick look at distributions of some key columns\n",
    "print(\"\\nValue counts for selected columns:\")\n",
    "for col in [\n",
    "    \"eOutcome_01\", \"eOutcome_02\", \"eMedications_03\", \n",
    "    \"eSituation_11\", \"eSituation_12\", \"injury_cause_count\",\n",
    "    \"injury_risk_flag\", \"trauma_criteria_count\", \"alcohol_drug_use_flag\"\n",
    "]:\n",
    "    if col in events_df.columns:\n",
    "        print(f\"\\n--- {col} ---\")\n",
    "        print(events_df[col].value_counts(dropna=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49f51a21-b458-4583-a541-52d18f26e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../data/interim/opioid_cases_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the final dataset\n",
    "events_df.to_csv(\"../data/interim/opioid_cases_full.csv\", index=False)\n",
    "print(\"Saved to ../data/interim/opioid_cases_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84c7c5-462a-4c05-94e7-71b21185c5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb8218-1fad-4943-b526-b8bcaeb89b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0841f4-5b1c-4d96-aee9-138f7c90e7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae40da-449a-4bad-a6c3-65cbb670f43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d3ad806-9565-4be8-8a8c-729a880d451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Cause of Injury: 545it [00:34, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cause of Injury records loaded: 271756\n"
     ]
    }
   ],
   "source": [
    "cause_injury_path = \"../data/raw/FACTPCRCAUSEOFINJURY.txt\"\n",
    "\n",
    "cause_injury_chunks = []\n",
    "\n",
    "with open(cause_injury_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Cause of Injury\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        \n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            cause_injury_chunks.append(filtered)\n",
    "\n",
    "cause_injury_df = pd.concat(cause_injury_chunks, ignore_index=True)\n",
    "print(\"Cause of Injury records loaded:\", len(cause_injury_df))\n",
    "\n",
    "cause_injury_agg = (\n",
    "    cause_injury_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        cause_injury_first=(\"eInjury_01\", \"first\"),\n",
    "        cause_injury_count=(\"eInjury_01\", \"count\"),\n",
    "        unique_causes=(\"eInjury_01\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c3d9f5-d136-4da7-8259-420c3a6dc189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in ../data/raw/FACTPCRPRIMARYSYMPTOM.txt:\n",
      "- PcrKey\n",
      "- eSituation_09\n",
      "Columns in ../data/raw/FACTPCRADDITIONALSYMPTOM.txt:\n",
      "- PcrKey\n",
      "- eSituation_10\n",
      "Columns in ../data/raw/FACTPCRCAUSEOFINJURY.txt:\n",
      "- PcrKey\n",
      "- eInjury_01\n",
      "Columns in ../data/raw/FACTPCRINJURYRISKFACTOR.txt:\n",
      "- PcrKey\n",
      "- eInjury_04\n",
      "Columns in ../data/raw/FACTPCRTRAUMACRITERIA.txt:\n",
      "- PcrKey\n",
      "- eInjury_03\n",
      "Columns in ../data/raw/FACTPCRBARRIERTOCARE.txt:\n",
      "- PcrKey\n",
      "- eHistory_01\n",
      "Columns in ../data/raw/FACTPCRDISPATCHDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_08\n",
      "Columns in ../data/raw/FACTPCRRESPONSEDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_09\n",
      "Columns in ../data/raw/FACTPCRSCENEDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_10\n",
      "Columns in ../data/raw/FACTPCRTRANSPORTDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_11\n",
      "Columns in ../data/raw/FACTPCRTURNAROUNDDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_12\n",
      "Columns in ../data/raw/FACTPCRDESTINATIONREASON.txt:\n",
      "- PcrKey\n",
      "- eDisposition_20\n",
      "Columns in ../data/raw/FACTPCRDESTINATIONTEAM.txt:\n",
      "- eDisposition_25\n",
      "- PcrKey\n",
      "- eDisposition_24\n",
      "Columns in ../data/raw/FACTPCRWORKRELATEDEXPOSURE.txt:\n",
      "- PcrKey\n",
      "- eOther_05\n"
     ]
    }
   ],
   "source": [
    "def inspect_file_columns(file_path, n_rows=5):\n",
    "    \"\"\"\n",
    "    Quickly read the first few rows of a pipe-delimited file\n",
    "    and print cleaned column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        delimiter=\"|\",\n",
    "        nrows=n_rows,\n",
    "        dtype=str\n",
    "    )\n",
    "    # Clean columns\n",
    "    df.columns = df.columns.str.strip(\" ~'\")\n",
    "    print(f\"Columns in {file_path}:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}\")\n",
    "\n",
    "# Symptoms\n",
    "inspect_file_columns(\"../data/raw/FACTPCRPRIMARYSYMPTOM.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRADDITIONALSYMPTOM.txt\")\n",
    "\n",
    "# Cause of injury\n",
    "inspect_file_columns(\"../data/raw/FACTPCRCAUSEOFINJURY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRINJURYRISKFACTOR.txt\")\n",
    "\n",
    "# Trauma criteria\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTRAUMACRITERIA.txt\")\n",
    "\n",
    "# Barriers to care\n",
    "inspect_file_columns(\"../data/raw/FACTPCRBARRIERTOCARE.txt\")\n",
    "\n",
    "# Response and transport delays\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDISPATCHDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRRESPONSEDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRSCENEDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTRANSPORTDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTURNAROUNDDELAY.txt\")\n",
    "\n",
    "# Destination details\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDESTINATIONREASON.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDESTINATIONTEAM.txt\")\n",
    "\n",
    "# Work related exposure\n",
    "inspect_file_columns(\"../data/raw/FACTPCRWORKRELATEDEXPOSURE.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34e86d6a-3c45-4440-8af5-ff21e147a0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Injury Risk Factor: 543it [00:35, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injury Risk Factor records loaded: 271206\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['eInjury_03'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m injury_risk_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(injury_risk_chunks, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInjury Risk Factor records loaded:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(injury_risk_df))\n\u001b[1;32m     20\u001b[0m injury_risk_agg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 21\u001b[0m     \u001b[43minjury_risk_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPcrKey\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43minjury_risk_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meInjury_03\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43munique_injury_risks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meInjury_03\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnunique\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[1;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[1;32m   1606\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1607\u001b[0m ):\n\u001b[0;32m-> 1608\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1611\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/apply.py:462\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m is_groupby \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[1;32m    461\u001b[0m func \u001b[38;5;241m=\u001b[39m cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[0;32m--> 462\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m is_non_unique_col \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    465\u001b[0m     selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(selected_obj\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/apply.py:663\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m     cols \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;241m.\u001b[39mdifference(obj\u001b[38;5;241m.\u001b[39mcolumns, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    665\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['eInjury_03'] do not exist\""
     ]
    }
   ],
   "source": [
    "injury_risk_path = \"../data/raw/FACTPCRINJURYRISKFACTOR.txt\"\n",
    "\n",
    "injury_risk_chunks = []\n",
    "\n",
    "with open(injury_risk_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Injury Risk Factor\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        \n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            injury_risk_chunks.append(filtered)\n",
    "\n",
    "injury_risk_df = pd.concat(injury_risk_chunks, ignore_index=True)\n",
    "print(\"Injury Risk Factor records loaded:\", len(injury_risk_df))\n",
    "\n",
    "injury_risk_agg = (\n",
    "    injury_risk_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        injury_risk_count=(\"eInjury_03\", \"count\"),\n",
    "        unique_injury_risks=(\"eInjury_03\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ae4c5b-f04a-4d95-9fae-0cbbf09183f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Event Records: 542it [04:33,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event records loaded: 271206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to event file\n",
    "event_path = \"../data/raw/pub_pcrevents_cp25.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "event_chunks = []\n",
    "\n",
    "with open(event_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Event Records\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            event_chunks.append(filtered)\n",
    "\n",
    "event_df = pd.concat(event_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Event records loaded:\", len(event_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf5fb5b6-6df8-41a1-8a79-08b7040ff065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Vitals: 1648it [05:29,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vitals aggregated: (271071, 64)\n"
     ]
    }
   ],
   "source": [
    "# Path to Vitals file\n",
    "vitals_path = \"../data/raw/FACTPCRVITAL.txt\"\n",
    "\n",
    "# Define vitals columns to extract and aggregate\n",
    "vital_cols = {\n",
    "    \"HeartRate\": \"eVitals_10\",\n",
    "    \"RespRate\": \"eVitals_14\",\n",
    "    \"SystolicBP\": \"eVitals_06\",\n",
    "    \"SpO2\": \"eVitals_12\",\n",
    "    \"BGL\": \"eVitals_18\",\n",
    "    \"ETCO2\": \"eVitals_16\",\n",
    "    \"GCS_Eye\": \"eVitals_19\",\n",
    "    \"GCS_Verbal\": \"eVitals_20\",\n",
    "    \"GCS_Motor\": \"eVitals_21\"\n",
    "}\n",
    "\n",
    "# Prepare chunks\n",
    "vitals_chunks = []\n",
    "\n",
    "with open(vitals_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Vitals\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)].copy()\n",
    "        if not filtered.empty:\n",
    "            for name, col in vital_cols.items():\n",
    "                if col in filtered.columns:\n",
    "                    extracted = filtered[col].str.extract(r\"(\\d+\\.?\\d*)\")[0]\n",
    "                    filtered[name] = pd.to_numeric(extracted, errors=\"coerce\").where(lambda x: x < 1000)\n",
    "            vitals_chunks.append(filtered[[\"PcrKey\"] + list(vital_cols.keys())])\n",
    "\n",
    "# Combine all\n",
    "vitals_df = pd.concat(vitals_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate per PcrKey\n",
    "vitals_agg = (\n",
    "    vitals_df.groupby(\"PcrKey\")[list(vital_cols.keys())]\n",
    "    .agg([\"first\", \"last\", \"min\", \"max\", \"mean\", \"std\", \"count\"])\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "vitals_agg.columns = [\"_\".join(col).strip() for col in vitals_agg.columns.values]\n",
    "vitals_agg = vitals_agg.reset_index()\n",
    "\n",
    "print(\"Vitals aggregated:\", vitals_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "573bb529-9762-48d4-aba6-83521c814b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Medications: 628it [01:44,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medications records loaded: 456070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Medications file\n",
    "meds_path = \"../data/raw/FACTPCRMEDICATION.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "meds_chunks = []\n",
    "\n",
    "with open(meds_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Medications\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        chunk[\"eMedications_03Descr\"] = chunk[\"eMedications_03Descr\"].str.strip().str.lower()\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            meds_chunks.append(filtered)\n",
    "\n",
    "# Combine all\n",
    "meds_df = pd.concat(meds_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Medications records loaded:\", len(meds_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2338e545-3a8d-426c-9e8d-a213e1b65b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medications aggregated: (271206, 7)\n"
     ]
    }
   ],
   "source": [
    "# Flag Naloxone\n",
    "naloxone_flag = meds_df[\"eMedications_03Descr\"].str.contains(\"naloxone|narcan\", na=False)\n",
    "\n",
    "# Aggregate medication info\n",
    "meds_agg = (\n",
    "    meds_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        total_meds=(\"eMedications_03\", \"count\"),\n",
    "        unique_meds=(\"eMedications_03\", \"nunique\"),\n",
    "        naloxone_doses=(\"eMedications_03Descr\", lambda x: x.str.contains(\"naloxone|narcan\", na=False).sum()),\n",
    "        naloxone_flag=(\"eMedications_03Descr\", lambda x: x.str.contains(\"naloxone|narcan\", na=False).any()),\n",
    "        first_route=(\"eMedications_07\", \"first\"),\n",
    "        first_response=(\"eMedications_10\", \"first\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Medications aggregated:\", meds_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e52deeba-5bdf-4ed7-be91-06c0c7523582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Procedures: 934it [01:50,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedures records loaded: 587755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Procedures file\n",
    "proc_path = \"../data/raw/FACTPCRPROCEDURE.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "proc_chunks = []\n",
    "\n",
    "with open(proc_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Procedures\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            proc_chunks.append(filtered)\n",
    "\n",
    "# Combine all\n",
    "proc_df = pd.concat(proc_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Procedures records loaded:\", len(proc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b630d80f-d28f-4696-96e4-5fdb2b0b252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedures aggregated: (271206, 6)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate procedures per PcrKey\n",
    "proc_agg = (\n",
    "    proc_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        procedure_count=(\"eProcedures_03\", \"count\"),\n",
    "        unique_procedures=(\"eProcedures_03\", \"nunique\"),\n",
    "        first_procedure=(\"eProcedures_03\", \"first\"),\n",
    "        all_procedures=(\"eProcedures_03\", lambda x: list(x.dropna()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Optionally stringify list for easier storage\n",
    "proc_agg[\"all_procedures_str\"] = proc_agg[\"all_procedures\"].apply(lambda x: \"|\".join(x) if x else \"\")\n",
    "\n",
    "print(\"Procedures aggregated:\", proc_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10e34f16-38cb-4411-afcf-4abba7e21559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading CPR Records: 546it [00:34, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPR records loaded: 275322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to CPR records\n",
    "cpr_path = \"../data/raw/FACTPCRARRESTCPRPROVIDED.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "cpr_chunks = []\n",
    "\n",
    "with open(cpr_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading CPR Records\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            cpr_chunks.append(filtered)\n",
    "\n",
    "cpr_df = pd.concat(cpr_chunks, ignore_index=True)\n",
    "\n",
    "print(\"CPR records loaded:\", len(cpr_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddafba95-d190-4349-924a-91adfecc02d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPR aggregated: (271206, 4)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate CPR info\n",
    "cpr_agg = (\n",
    "    cpr_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        cpr_given=(\"eArrest_09\", lambda x: True),\n",
    "        bystander_cpr=(\"eArrest_09\", lambda x: x.str.contains(\"BYSTANDER\", case=False, na=False).any()),\n",
    "        ems_cpr=(\"eArrest_09\", lambda x: x.str.contains(\"EMS|CREW|PROVIDER\", case=False, na=False).any())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"CPR aggregated:\", cpr_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9445c5cf-04dd-42aa-8adf-628c21fabebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ROSC Records: 543it [00:35, 15.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSC records loaded: 271712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to ROSC records\n",
    "rosc_path = \"../data/raw/FACTPCRARRESTROSC.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "rosc_chunks = []\n",
    "\n",
    "with open(rosc_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading ROSC Records\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            rosc_chunks.append(filtered)\n",
    "\n",
    "rosc_df = pd.concat(rosc_chunks, ignore_index=True)\n",
    "\n",
    "print(\"ROSC records loaded:\", len(rosc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c1db0a-badb-4f3a-a9ea-2bc5f2cf3935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSC aggregated: (271206, 2)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate ROSC flag\n",
    "rosc_agg = (\n",
    "    rosc_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        rosc_achieved=(\"eArrest_12\", lambda x: x.str.contains(\"YES\", case=False, na=False).any())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"ROSC aggregated:\", rosc_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fd3b1a3-faf6-4545-967e-51d5c8c3f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Computed Demographics: 542it [01:13,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics aggregated: (271206, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Computed Elements\n",
    "computed_path = \"../data/raw/ComputedElements.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "computed_chunks = []\n",
    "\n",
    "with open(computed_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Computed Demographics\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        # Check which columns exist\n",
    "        cols_available = [col for col in [\"PcrKey\", \"ePatient_15\", \"ePatient_16\"] if col in chunk.columns]\n",
    "        if not cols_available:\n",
    "            continue\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)][cols_available]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            computed_chunks.append(filtered)\n",
    "\n",
    "# Combine\n",
    "computed_df = pd.concat(computed_chunks, ignore_index=True)\n",
    "\n",
    "# Deduplicate\n",
    "demographics_agg = computed_df.drop_duplicates(subset=\"PcrKey\").copy()\n",
    "\n",
    "# Convert types if columns exist\n",
    "if \"ePatient_15\" in demographics_agg.columns:\n",
    "    demographics_agg[\"ePatient_15\"] = pd.to_numeric(demographics_agg[\"ePatient_15\"], errors=\"coerce\")\n",
    "\n",
    "if \"ePatient_16\" in demographics_agg.columns:\n",
    "    demographics_agg[\"ePatient_16\"] = demographics_agg[\"ePatient_16\"].astype(str).str.strip(\" ~\")\n",
    "\n",
    "print(\"Demographics aggregated:\", demographics_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "252fa5d1-e372-48bf-8b33-298b2b812a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Alcohol/Drug Use: 553it [00:34, 16.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol/Drug Use records loaded: 319541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Alcohol/Drug Use indicators\n",
    "drug_path = \"../data/raw/FACTPCRALCOHOLDRUGUSEINDICATOR.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "drug_chunks = []\n",
    "\n",
    "with open(drug_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Alcohol/Drug Use\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            drug_chunks.append(filtered)\n",
    "\n",
    "drug_df = pd.concat(drug_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Alcohol/Drug Use records loaded:\", len(drug_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e91a7434-96ba-4daa-87ab-436398b09673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol/Drug Use aggregated: (271206, 5)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate flags per PcrKey\n",
    "drug_agg = (\n",
    "    drug_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        use_flag_count=(\"eHistory_17\", \"count\"),\n",
    "        unique_use_flags=(\"eHistory_17\", \"nunique\"),\n",
    "        all_use_flags=(\"eHistory_17\", lambda x: list(x.dropna()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Stringify list for storage\n",
    "drug_agg[\"all_use_flags_str\"] = drug_agg[\"all_use_flags\"].apply(lambda x: \"|\".join(x) if x else \"\")\n",
    "\n",
    "print(\"Alcohol/Drug Use aggregated:\", drug_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7691d31a-ad71-45f8-a480-1d46bdf7e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Additional Symptoms: 580it [00:34, 16.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Symptoms records loaded: 306286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Additional Symptoms\n",
    "symptom_path = \"../data/raw/FACTPCRADDITIONALSYMPTOM.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "symptom_chunks = []\n",
    "\n",
    "with open(symptom_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Additional Symptoms\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            symptom_chunks.append(filtered)\n",
    "\n",
    "symptom_df = pd.concat(symptom_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Additional Symptoms records loaded:\", len(symptom_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc86e4f9-e8a8-413b-9db6-c6281a8f231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Symptoms aggregated: (267904, 5)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate per PcrKey\n",
    "symptom_agg = (\n",
    "    symptom_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        symptom_count=(\"eSituation_10\", \"count\"),\n",
    "        unique_symptoms=(\"eSituation_10\", \"nunique\"),\n",
    "        all_symptoms=(\"eSituation_10\", lambda x: list(x.dropna()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Stringify list for easier storage\n",
    "symptom_agg[\"all_symptoms_str\"] = symptom_agg[\"all_symptoms\"].apply(lambda x: \"|\".join(x) if x else \"\")\n",
    "\n",
    "print(\"Additional Symptoms aggregated:\", symptom_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c86ff5-4f92-4e3e-9c91-19275247c6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged vitals: now shape (271206, 110)\n",
      "Merged medications: now shape (271206, 116)\n",
      "Merged procedures: now shape (271206, 121)\n",
      "Merged cpr: now shape (271206, 124)\n",
      "Merged rosc: now shape (271206, 125)\n",
      "Merged demographics: now shape (271206, 125)\n",
      "Merged alcohol_drug_use: now shape (271206, 129)\n",
      "Merged additional_symptoms: now shape (271206, 133)\n",
      "All data merged and saved to opioid_cases_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Start with the Event DataFrame (1 row per PcrKey)\n",
    "df_merged = event_df.copy()\n",
    "\n",
    "# List of all aggregated DataFrames to merge\n",
    "feature_tables = {\n",
    "    \"vitals\": vitals_agg,\n",
    "    \"medications\": meds_agg,\n",
    "    \"procedures\": proc_agg,\n",
    "    \"cpr\": cpr_agg,\n",
    "    \"rosc\": rosc_agg,\n",
    "    \"demographics\": demographics_agg,\n",
    "    \"alcohol_drug_use\": drug_agg,\n",
    "    \"additional_symptoms\": symptom_agg\n",
    "}\n",
    "\n",
    "# Merge each\n",
    "for name, table in feature_tables.items():\n",
    "    df_merged = df_merged.merge(table, on=\"PcrKey\", how=\"left\")\n",
    "    print(f\"Merged {name}: now shape {df_merged.shape}\")\n",
    "\n",
    "# Save to CSV\n",
    "df_merged.to_csv(\"../data/interim/opioid_cases_full.csv\", index=False)\n",
    "\n",
    "print(\"All data merged and saved to opioid_cases_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ceaaf-4b73-4f6e-93e1-545fcaec81da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemsis-env",
   "language": "python",
   "name": "nemsis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
