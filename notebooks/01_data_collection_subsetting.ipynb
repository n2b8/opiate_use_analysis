{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2273e787-23cb-4c54-a406-a1f313207ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "933a599f-2f54-475e-9acc-cc82b9c911dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_file_columns(file_path, n_rows=5):\n",
    "    \"\"\"\n",
    "    Quickly read the first few rows of a pipe-delimited file\n",
    "    and print cleaned column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        delimiter=\"|\",\n",
    "        nrows=n_rows,\n",
    "        dtype=str\n",
    "    )\n",
    "    # Clean columns\n",
    "    df.columns = df.columns.str.strip(\" ~'\")\n",
    "    print(f\"Columns in {file_path}:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52169717-6cf8-4fc1-b850-0220e60e6a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in ../data/raw/FACTPCRPRIMARYIMPRESSION.txt:\n",
      "- PcrKey\n",
      "- eSituation_11\n",
      "Columns in ../data/raw/FACTPCRSECONDARYIMPRESSION.txt:\n",
      "- PcrKey\n",
      "- eSituation_12\n",
      "Columns in ../data/raw/FACTPCRPRIMARYSYMPTOM.txt:\n",
      "- PcrKey\n",
      "- eSituation_09\n",
      "Columns in ../data/raw/FACTPCRADDITIONALSYMPTOM.txt:\n",
      "- PcrKey\n",
      "- eSituation_10\n",
      "Columns in ../data/raw/FACTPCRCAUSEOFINJURY.txt:\n",
      "- PcrKey\n",
      "- eInjury_01\n",
      "Columns in ../data/raw/FACTPCRINJURYRISKFACTOR.txt:\n",
      "- PcrKey\n",
      "- eInjury_04\n",
      "Columns in ../data/raw/FACTPCRTRAUMACRITERIA.txt:\n",
      "- PcrKey\n",
      "- eInjury_03\n",
      "Columns in ../data/raw/FACTPCRBARRIERTOCARE.txt:\n",
      "- PcrKey\n",
      "- eHistory_01\n",
      "Columns in ../data/raw/FACTPCRDISPATCHDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_08\n",
      "Columns in ../data/raw/FACTPCRRESPONSEDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_09\n",
      "Columns in ../data/raw/FACTPCRSCENEDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_10\n",
      "Columns in ../data/raw/FACTPCRTRANSPORTDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_11\n",
      "Columns in ../data/raw/FACTPCRTURNAROUNDDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_12\n",
      "Columns in ../data/raw/FACTPCRDESTINATIONREASON.txt:\n",
      "- PcrKey\n",
      "- eDisposition_20\n",
      "Columns in ../data/raw/FACTPCRDESTINATIONTEAM.txt:\n",
      "- eDisposition_25\n",
      "- PcrKey\n",
      "- eDisposition_24\n",
      "Columns in ../data/raw/FACTPCRVITAL.txt:\n",
      "- PcrVitalKey\n",
      "- PcrKey\n",
      "- eVitals_06\n",
      "- eVitals_10\n",
      "- eVitals_12\n",
      "- eVitals_14\n",
      "- eVitals_16\n",
      "- eVitals_18\n",
      "- eVitals_27\n",
      "- eVitals_02\n",
      "- eVitals_04\n",
      "- eVitals_08\n",
      "- eVitals_26\n",
      "- eVitals_29\n",
      "- eVitals_30\n",
      "- eVitals_31\n",
      "- eVitals_19\n",
      "- eVitals_20\n",
      "- eVitals_21\n",
      "- eVitals_01\n",
      "Columns in ../data/raw/FACTPCRARRESTCPRPROVIDED.txt:\n",
      "- PcrKey\n",
      "- eArrest_09\n",
      "Columns in ../data/raw/FACTPCRARRESTRESUSCITATION.txt:\n",
      "- PcrKey\n",
      "- eArrest_03\n",
      "Columns in ../data/raw/FACTPCRARRESTRHYTHMDESTINATION.txt:\n",
      "- PcrKey\n",
      "- eArrest_17\n",
      "Columns in ../data/raw/FACTPCRARRESTROSC.txt:\n",
      "- PcrKey\n",
      "- eArrest_12\n",
      "Columns in ../data/raw/FACTPCRARRESTWITNESS.txt:\n",
      "- PcrKey\n",
      "- eArrest_04\n",
      "Columns in ../data/raw/FACTPCRMEDICATION.txt:\n",
      "- eMedications_01\n",
      "- PcrMedicationKey\n",
      "- PcrKey\n",
      "- eMedications_03\n",
      "- eMedications_03Descr\n",
      "- eMedications_05\n",
      "- eMedications_06\n",
      "- eMedications_07\n",
      "- eMedications_10\n",
      "- eMedications_02\n",
      "Columns in ../data/raw/FACTPCRPROCEDURE.txt:\n",
      "- PcrProcedureKey\n",
      "- PcrKey\n",
      "- eProcedures_03\n",
      "- eProcedures_05\n",
      "- eProcedures_06\n",
      "- eProcedures_08\n",
      "- eProcedures_10\n",
      "- eProcedures_02\n",
      "- eProcedures_01\n",
      "Columns in ../data/raw/FACTPCRPROTOCOL.txt:\n",
      "- PcrKey\n",
      "- eProtocols_01\n",
      "- eProtocols_02\n",
      "Columns in ../data/raw/FACTPCRADDITIONALRESPONSEMODE.txt:\n",
      "- PcrKey\n",
      "- eResponse_24\n",
      "Columns in ../data/raw/FACTPCRADDITIONALTRANSPORTMODE.txt:\n",
      "- PcrKey\n",
      "- eDisposition_18\n",
      "Columns in ../data/raw/FACTPCRALCOHOLDRUGUSEINDICATOR.txt:\n",
      "- PcrKey\n",
      "- eHistory_17\n",
      "Columns in ../data/raw/FACTPCRWORKRELATEDEXPOSURE.txt:\n",
      "- PcrKey\n",
      "- eOther_05\n",
      "Columns in ../data/raw/PCRPATIENTRACEGROUP.txt:\n",
      "- PcrPatientRaceGroupKey\n",
      "- PcrKey\n",
      "- ePatient_14\n",
      "Columns in ../data/raw/PCRPROCCOMPGROUP.txt:\n",
      "- PcrProcCompGroupKey\n",
      "- ProcedureKey\n",
      "- eProcedures_07\n",
      "Columns in ../data/raw/PCRMEDCOMPGROUP.txt:\n",
      "- PcrMedCompGroupKey\n",
      "- MedicationKey\n",
      "- eMedications_08\n",
      "Columns in ../data/raw/PCRVITALECGGROUP.txt:\n",
      "- PcrVitalECGGroupKey\n",
      "- VitalKey\n",
      "- eVitals_03\n",
      "Columns in ../data/raw/PCRVITALECGINTERPRETATIONGROUP.txt:\n",
      "- PcrVitalECGInterpretationGroupKe\n",
      "- VitalKey\n",
      "- eVitals_05\n",
      "Columns in ../data/raw/PCRVITALGLASGOWQUALIFIERGROUP.txt:\n",
      "- PcrVitalECGGroupKey\n",
      "- VitalKey\n",
      "- eVitals_22\n",
      "Columns in ../data/raw/pub_pcrevents_cp25.txt:\n",
      "- PcrKey\n",
      "- eDispatch_01\n",
      "- eDispatch_02\n",
      "- eArrest_14\n",
      "- eArrest_01\n",
      "- eArrest_02\n",
      "- eArrest_05\n",
      "- eArrest_07\n",
      "- eArrest_11\n",
      "- eArrest_16\n",
      "- eArrest_18\n",
      "- eDisposition_12\n",
      "- eDisposition_19\n",
      "- eDisposition_16\n",
      "- eDisposition_21\n",
      "- eDisposition_22\n",
      "- eDisposition_23\n",
      "- eOutcome_01\n",
      "- eOutcome_02\n",
      "- ePatient_15\n",
      "- ePatient_16\n",
      "- ePayment_01\n",
      "- ePayment_50\n",
      "- eResponse_05\n",
      "- eResponse_07\n",
      "- eResponse_15\n",
      "- eResponse_23\n",
      "- eScene_01\n",
      "- eScene_06\n",
      "- eScene_07\n",
      "- eScene_08\n",
      "- eScene_09\n",
      "- eSituation_02\n",
      "- eSituation_07\n",
      "- eSituation_08\n",
      "- eSituation_13\n",
      "- eSituation_01\n",
      "- eTimes_01\n",
      "- eTimes_03\n",
      "- eTimes_05\n",
      "- eTimes_06\n",
      "- eTimes_07\n",
      "- eTimes_09\n",
      "- eTimes_11\n",
      "- eTimes_12\n",
      "- eTimes_13\n",
      "- eDisposition_17\n"
     ]
    }
   ],
   "source": [
    "# Provider Impressions\n",
    "inspect_file_columns(\"../data/raw/FACTPCRPRIMARYIMPRESSION.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRSECONDARYIMPRESSION.txt\")\n",
    "\n",
    "# Symptoms\n",
    "inspect_file_columns(\"../data/raw/FACTPCRPRIMARYSYMPTOM.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRADDITIONALSYMPTOM.txt\")\n",
    "\n",
    "# Cause of Injury and Trauma\n",
    "inspect_file_columns(\"../data/raw/FACTPCRCAUSEOFINJURY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRINJURYRISKFACTOR.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTRAUMACRITERIA.txt\")\n",
    "\n",
    "# Barriers to Care\n",
    "inspect_file_columns(\"../data/raw/FACTPCRBARRIERTOCARE.txt\")\n",
    "\n",
    "# Delay Types\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDISPATCHDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRRESPONSEDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRSCENEDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTRANSPORTDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTURNAROUNDDELAY.txt\")\n",
    "\n",
    "# Destination Details\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDESTINATIONREASON.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDESTINATIONTEAM.txt\")\n",
    "\n",
    "# Vitals\n",
    "inspect_file_columns(\"../data/raw/FACTPCRVITAL.txt\")\n",
    "\n",
    "# Arrest & CPR\n",
    "inspect_file_columns(\"../data/raw/FACTPCRARRESTCPRPROVIDED.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRARRESTRESUSCITATION.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRARRESTRHYTHMDESTINATION.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRARRESTROSC.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRARRESTWITNESS.txt\")\n",
    "\n",
    "# Medication & Procedure\n",
    "inspect_file_columns(\"../data/raw/FACTPCRMEDICATION.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRPROCEDURE.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRPROTOCOL.txt\")\n",
    "\n",
    "# Additional Modes\n",
    "inspect_file_columns(\"../data/raw/FACTPCRADDITIONALRESPONSEMODE.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRADDITIONALTRANSPORTMODE.txt\")\n",
    "\n",
    "# Alcohol/Drug Use Indicator\n",
    "inspect_file_columns(\"../data/raw/FACTPCRALCOHOLDRUGUSEINDICATOR.txt\")\n",
    "\n",
    "# Work Related Exposure\n",
    "inspect_file_columns(\"../data/raw/FACTPCRWORKRELATEDEXPOSURE.txt\")\n",
    "\n",
    "# Patient Groupings\n",
    "inspect_file_columns(\"../data/raw/PCRPATIENTRACEGROUP.txt\")\n",
    "\n",
    "# Procedure/ECG Groupings\n",
    "inspect_file_columns(\"../data/raw/PCRPROCCOMPGROUP.txt\")\n",
    "inspect_file_columns(\"../data/raw/PCRMEDCOMPGROUP.txt\")\n",
    "inspect_file_columns(\"../data/raw/PCRVITALECGGROUP.txt\")\n",
    "inspect_file_columns(\"../data/raw/PCRVITALECGINTERPRETATIONGROUP.txt\")\n",
    "inspect_file_columns(\"../data/raw/PCRVITALGLASGOWQUALIFIERGROUP.txt\")\n",
    "\n",
    "# Core Events Table\n",
    "inspect_file_columns(\"../data/raw/pub_pcrevents_cp25.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67456394-cd26-41fc-af0b-399bd364463b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Primary Impressions: 530it [00:22, 23.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary impressions matched: 231354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define target ICD-10 prefixes\n",
    "target_prefixes = (\"T40\", \"F11\")\n",
    "\n",
    "# Path to primary impressions file\n",
    "primary_path = \"../data/raw/FACTPCRPRIMARYIMPRESSION.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "primary_chunks = []\n",
    "\n",
    "with open(primary_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Processing Primary Impressions\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"eSituation_11\"] = chunk[\"eSituation_11\"].str.strip(\" ~\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        mask = chunk[\"eSituation_11\"].str.startswith(target_prefixes)\n",
    "        filtered = chunk.loc[mask, [\"PcrKey\"]]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            primary_chunks.append(filtered)\n",
    "\n",
    "primary_df = pd.concat(primary_chunks, ignore_index=True)\n",
    "print(\"Primary impressions matched:\", len(primary_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42d367fc-6922-4a93-8705-3648ae4a8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Secondary Impressions: 544it [00:23, 23.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secondary impressions matched: 42853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to secondary impressions file\n",
    "secondary_path = \"../data/raw/FACTPCRSECONDARYIMPRESSION.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "secondary_chunks = []\n",
    "\n",
    "with open(secondary_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Processing Secondary Impressions\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"eSituation_12\"] = chunk[\"eSituation_12\"].str.strip(\" ~\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        mask = chunk[\"eSituation_12\"].str.startswith(target_prefixes)\n",
    "        filtered = chunk.loc[mask, [\"PcrKey\"]]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            secondary_chunks.append(filtered)\n",
    "\n",
    "secondary_df = pd.concat(secondary_chunks, ignore_index=True)\n",
    "print(\"Secondary impressions matched:\", len(secondary_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5698a695-cf23-40a5-8e95-f78d438841b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique opioid-related cases: 271206\n"
     ]
    }
   ],
   "source": [
    "# Combine and deduplicate primary and secondary impression matches\n",
    "opioid_cases = pd.concat([primary_df, secondary_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "# Optionally, turn it into a set for fast lookups\n",
    "opioid_pcr_keys = set(opioid_cases[\"PcrKey\"])\n",
    "print(\"Total unique opioid-related cases:\", len(opioid_pcr_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d8ab3cf-ba62-48ad-b502-cc28fef66b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering pub_pcrevents_cp25.txt: 542it [04:18,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered events records: 271206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load core events table\n",
    "events_path = \"../data/raw/pub_pcrevents_cp25.txt\"\n",
    "events_chunks = []\n",
    "\n",
    "with open(events_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Filtering pub_pcrevents_cp25.txt\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "        if not filtered.empty:\n",
    "            events_chunks.append(filtered)\n",
    "\n",
    "events_df = pd.concat(events_chunks, ignore_index=True)\n",
    "print(\"Filtered events records:\", len(events_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "560eeb85-996c-4e9d-ad35-51a3155a80f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Primary Symptom: 534it [00:35, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged primary symptom. New shape: (271206, 48)\n"
     ]
    }
   ],
   "source": [
    "# Load and aggregate eSituation_09 (Primary Symptom)\n",
    "symptom_path = \"../data/raw/FACTPCRPRIMARYSYMPTOM.txt\"\n",
    "symptom_chunks = []\n",
    "\n",
    "with open(symptom_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Processing Primary Symptom\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        chunk[\"eSituation_09\"] = chunk[\"eSituation_09\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "        if not filtered.empty:\n",
    "            symptom_chunks.append(filtered[[\"PcrKey\", \"eSituation_09\"]])\n",
    "\n",
    "symptom_df = pd.concat(symptom_chunks, ignore_index=True)\n",
    "\n",
    "# Merge into events_df (one-to-one)\n",
    "events_df = events_df.merge(symptom_df, on=\"PcrKey\", how=\"left\")\n",
    "print(\"Merged primary symptom. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9dc00fd-81aa-4309-8fb2-43518e603bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Associated Symptoms: 580it [00:38, 15.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged associated symptoms. New shape: (271206, 50)\n"
     ]
    }
   ],
   "source": [
    "# Load and aggregate eSituation_10 (Other Associated Symptoms)\n",
    "assoc_path = \"../data/raw/FACTPCRADDITIONALSYMPTOM.txt\"\n",
    "assoc_chunks = []\n",
    "\n",
    "with open(assoc_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Processing Associated Symptoms\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        chunk[\"eSituation_10\"] = chunk[\"eSituation_10\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "        if not filtered.empty:\n",
    "            assoc_chunks.append(filtered[[\"PcrKey\", \"eSituation_10\"]])\n",
    "\n",
    "assoc_df = pd.concat(assoc_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate: count and nunique per PcrKey\n",
    "assoc_agg = (\n",
    "    assoc_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        assoc_symptom_count=(\"eSituation_10\", \"count\"),\n",
    "        assoc_symptom_unique=(\"eSituation_10\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge\n",
    "events_df = events_df.merge(assoc_agg, on=\"PcrKey\", how=\"left\")\n",
    "print(\"Merged associated symptoms. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d332e356-700b-493b-94e7-04ebaf646b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reloading Primary Impressions: 530it [00:33, 15.79it/s]\n",
      "Reloading Secondary Impressions: 544it [00:35, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged primary impression. New shape: (271206, 51)\n",
      "Merged secondary impression. New shape: (281813, 52)\n"
     ]
    }
   ],
   "source": [
    "# Reload and merge full Primary and Secondary Impressions\n",
    "\n",
    "primary_chunks = []\n",
    "secondary_chunks = []\n",
    "\n",
    "# Reload Primary Impressions\n",
    "with open(\"../data/raw/FACTPCRPRIMARYIMPRESSION.txt\", \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Reloading Primary Impressions\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        chunk[\"eSituation_11\"] = chunk[\"eSituation_11\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "        if not filtered.empty:\n",
    "            primary_chunks.append(filtered[[\"PcrKey\", \"eSituation_11\"]])\n",
    "\n",
    "# Reload Secondary Impressions\n",
    "with open(\"../data/raw/FACTPCRSECONDARYIMPRESSION.txt\", \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Reloading Secondary Impressions\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        chunk[\"eSituation_12\"] = chunk[\"eSituation_12\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "        if not filtered.empty:\n",
    "            secondary_chunks.append(filtered[[\"PcrKey\", \"eSituation_12\"]])\n",
    "\n",
    "# Concatenate and merge\n",
    "primary_imp_df = pd.concat(primary_chunks, ignore_index=True)\n",
    "secondary_imp_df = pd.concat(secondary_chunks, ignore_index=True)\n",
    "\n",
    "events_df = events_df.merge(primary_imp_df, on=\"PcrKey\", how=\"left\")\n",
    "print(\"Merged primary impression. New shape:\", events_df.shape)\n",
    "\n",
    "events_df = events_df.merge(secondary_imp_df, on=\"PcrKey\", how=\"left\")\n",
    "print(\"Merged secondary impression. New shape:\", events_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e977938-3ff3-4699-a96d-512892d42974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eInjury_01 (Cause of Injury): 545it [00:18, 29.59it/s]\n",
      "Loading eInjury_04 (Risk Factor): 543it [00:18, 29.93it/s]\n",
      "Loading eInjury_03 (Trauma Criteria): 543it [00:18, 29.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injury-related features merged. New shape: (281813, 55)\n"
     ]
    }
   ],
   "source": [
    "# Loading Injury Data\n",
    "\n",
    "# Paths to source files\n",
    "cause_path = \"../data/raw/FACTPCRCAUSEOFINJURY.txt\"\n",
    "risk_path = \"../data/raw/FACTPCRINJURYRISKFACTOR.txt\"\n",
    "trauma_path = \"../data/raw/FACTPCRTRAUMACRITERIA.txt\"\n",
    "\n",
    "# Initialize lists\n",
    "cause_chunks = []\n",
    "risk_chunks = []\n",
    "trauma_chunks = []\n",
    "\n",
    "# Load Cause of Injury (eInjury_01)\n",
    "with open(cause_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eInjury_01 (Cause of Injury)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eInjury_01\"] = chunk[\"eInjury_01\"].str.strip(\" ~'\")\n",
    "        cause_chunks.append(chunk[[\"PcrKey\", \"eInjury_01\"]])\n",
    "\n",
    "cause_df = pd.concat(cause_chunks, ignore_index=True)\n",
    "\n",
    "# Load Injury Risk Factor (eInjury_04)\n",
    "with open(risk_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eInjury_04 (Risk Factor)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eInjury_04\"] = chunk[\"eInjury_04\"].str.strip(\" ~'\")\n",
    "        risk_chunks.append(chunk[[\"PcrKey\", \"eInjury_04\"]])\n",
    "\n",
    "risk_df = pd.concat(risk_chunks, ignore_index=True)\n",
    "\n",
    "# Load Trauma Criteria (eInjury_03)\n",
    "with open(trauma_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eInjury_03 (Trauma Criteria)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eInjury_03\"] = chunk[\"eInjury_03\"].str.strip(\" ~'\")\n",
    "        trauma_chunks.append(chunk[[\"PcrKey\", \"eInjury_03\"]])\n",
    "\n",
    "trauma_df = pd.concat(trauma_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate each to 1 row per PcrKey\n",
    "cause_agg = cause_df.groupby(\"PcrKey\").agg(\n",
    "    injury_cause_count=(\"eInjury_01\", \"nunique\")\n",
    ").reset_index()\n",
    "\n",
    "risk_agg = (\n",
    "    risk_df.dropna(subset=[\"eInjury_04\"])\n",
    "    .groupby(\"PcrKey\")\n",
    "    .size()\n",
    "    .reset_index(name=\"injury_risk_flag\")\n",
    ")\n",
    "risk_agg[\"injury_risk_flag\"] = 1  # presence = 1\n",
    "\n",
    "trauma_agg = trauma_df.groupby(\"PcrKey\").agg(\n",
    "    trauma_criteria_count=(\"eInjury_03\", \"nunique\")\n",
    ").reset_index()\n",
    "\n",
    "# Merge into events_df\n",
    "events_df = events_df.merge(cause_agg, on=\"PcrKey\", how=\"left\")\n",
    "events_df = events_df.merge(risk_agg, on=\"PcrKey\", how=\"left\")\n",
    "events_df = events_df.merge(trauma_agg, on=\"PcrKey\", how=\"left\")\n",
    "\n",
    "# Fill missing with 0\n",
    "events_df[\"injury_cause_count\"] = events_df[\"injury_cause_count\"].fillna(0).astype(int)\n",
    "events_df[\"injury_risk_flag\"] = events_df[\"injury_risk_flag\"].fillna(0).astype(int)\n",
    "events_df[\"trauma_criteria_count\"] = events_df[\"trauma_criteria_count\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"Injury-related features merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a0d8baa-e70a-43f0-8710-8b82782a5986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eHistory_01 (Barriers to Care): 550it [00:38, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barriers to care merged. New shape: (281813, 58)\n"
     ]
    }
   ],
   "source": [
    "# Load and process eHistory_01 (Barriers to Patient Care)\n",
    "barrier_path = \"../data/raw/FACTPCRBARRIERTOCARE.txt\"\n",
    "barrier_chunks = []\n",
    "\n",
    "with open(barrier_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eHistory_01 (Barriers to Care)\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eHistory_01\"] = chunk[\"eHistory_01\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            barrier_chunks.append(filtered[[\"PcrKey\", \"eHistory_01\"]])\n",
    "\n",
    "barrier_df = pd.concat(barrier_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate\n",
    "barrier_agg = (\n",
    "    barrier_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        barrier_count=(\"eHistory_01\", \"count\"),\n",
    "        barrier_unique=(\"eHistory_01\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "barrier_agg[\"barrier_present\"] = 1\n",
    "\n",
    "# Merge into main dataset\n",
    "events_df = events_df.merge(barrier_agg, on=\"PcrKey\", how=\"left\")\n",
    "\n",
    "# Fill NA with zeros\n",
    "events_df[\"barrier_count\"] = events_df[\"barrier_count\"].fillna(0).astype(int)\n",
    "events_df[\"barrier_unique\"] = events_df[\"barrier_unique\"].fillna(0).astype(int)\n",
    "events_df[\"barrier_present\"] = events_df[\"barrier_present\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"Barriers to care merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59f0e6e5-d506-4917-b09e-e9fb8649afaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eResponse_08 from ../data/raw/FACTPCRDISPATCHDELAY.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eResponse_08: 543it [00:37, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eResponse_09 from ../data/raw/FACTPCRRESPONSEDELAY.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eResponse_09: 545it [00:39, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eResponse_10 from ../data/raw/FACTPCRSCENEDELAY.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eResponse_10: 546it [00:41, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eResponse_11 from ../data/raw/FACTPCRTRANSPORTDELAY.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eResponse_11: 544it [00:40, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eResponse_12 from ../data/raw/FACTPCRTURNAROUNDDELAY.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eResponse_12: 552it [00:41, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All delay types merged. New shape: (281813, 73)\n"
     ]
    }
   ],
   "source": [
    "# Delay files and their corresponding column names\n",
    "delay_sources = {\n",
    "    \"../data/raw/FACTPCRDISPATCHDELAY.txt\": \"eResponse_08\",\n",
    "    \"../data/raw/FACTPCRRESPONSEDELAY.txt\": \"eResponse_09\",\n",
    "    \"../data/raw/FACTPCRSCENEDELAY.txt\": \"eResponse_10\",\n",
    "    \"../data/raw/FACTPCRTRANSPORTDELAY.txt\": \"eResponse_11\",\n",
    "    \"../data/raw/FACTPCRTURNAROUNDDELAY.txt\": \"eResponse_12\",\n",
    "}\n",
    "\n",
    "# Loop through each and join delay features\n",
    "for file_path, col in delay_sources.items():\n",
    "    print(f\"Processing {col} from {file_path}\")\n",
    "    delay_chunks = []\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for chunk in tqdm(\n",
    "            pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "            desc=f\"Loading {col}\"\n",
    "        ):\n",
    "            chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "            chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "            chunk[col] = chunk[col].str.strip(\" ~'\")\n",
    "\n",
    "            filtered = chunk[chunk[\"PcrKey\"].isin(opioid_pcr_keys)]\n",
    "            if not filtered.empty:\n",
    "                delay_chunks.append(filtered[[\"PcrKey\", col]])\n",
    "\n",
    "    delay_df = pd.concat(delay_chunks, ignore_index=True)\n",
    "\n",
    "    # Aggregate delay types per PcrKey\n",
    "    delay_agg = (\n",
    "        delay_df.groupby(\"PcrKey\")[col]\n",
    "        .agg([\n",
    "            (\"{}_count\".format(col), \"count\"),\n",
    "            (\"{}_unique\".format(col), \"nunique\")\n",
    "        ])\n",
    "        .reset_index()\n",
    "    )\n",
    "    delay_agg[\"{}_present\".format(col)] = 1\n",
    "\n",
    "    # Merge into main events_df\n",
    "    events_df = events_df.merge(delay_agg, on=\"PcrKey\", how=\"left\")\n",
    "\n",
    "    # Fill missing values\n",
    "    events_df[\"{}_count\".format(col)] = events_df[\"{}_count\".format(col)].fillna(0).astype(int)\n",
    "    events_df[\"{}_unique\".format(col)] = events_df[\"{}_unique\".format(col)].fillna(0).astype(int)\n",
    "    events_df[\"{}_present\".format(col)] = events_df[\"{}_present\".format(col)].fillna(0).astype(int)\n",
    "\n",
    "print(\"All delay types merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a467c8ac-5103-4405-b6d3-2f399fbcf17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eDisposition_20 from ../data/raw/FACTPCRDESTINATIONREASON.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eDisposition_20: 582it [00:32, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing eDisposition_24 & eDisposition_25 from ../data/raw/FACTPCRDESTINATIONTEAM.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading eDisposition_24 & 25: 543it [00:40, 13.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destination-related features merged. New shape: (297762, 76)\n"
     ]
    }
   ],
   "source": [
    "# Load Destination Reason and Alert Info\n",
    "\n",
    "# File paths\n",
    "dest_reason_path = \"../data/raw/FACTPCRDESTINATIONREASON.txt\"\n",
    "dest_team_path = \"../data/raw/FACTPCRDESTINATIONTEAM.txt\"\n",
    "\n",
    "# Chunks\n",
    "reason_chunks = []\n",
    "team_chunks = []\n",
    "\n",
    "# eDisposition_20 - Reason for choosing destination\n",
    "print(f\"Processing eDisposition_20 from {dest_reason_path}\")\n",
    "with open(dest_reason_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eDisposition_20\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eDisposition_20\"] = chunk[\"eDisposition_20\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "        if not filtered.empty:\n",
    "            reason_chunks.append(filtered[[\"PcrKey\", \"eDisposition_20\"]])\n",
    "\n",
    "reason_df = pd.concat(reason_chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "# eDisposition_24 and eDisposition_25 - Alert type and timestamp\n",
    "print(f\"Processing eDisposition_24 & eDisposition_25 from {dest_team_path}\")\n",
    "with open(dest_team_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading eDisposition_24 & 25\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eDisposition_24\"] = chunk[\"eDisposition_24\"].str.strip(\" ~'\")\n",
    "        chunk[\"eDisposition_25\"] = chunk[\"eDisposition_25\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "        if not filtered.empty:\n",
    "            team_chunks.append(filtered[[\"PcrKey\", \"eDisposition_24\", \"eDisposition_25\"]])\n",
    "\n",
    "team_df = pd.concat(team_chunks, ignore_index=True)\n",
    "\n",
    "# Merge into events_df\n",
    "events_df = events_df.merge(reason_df, on=\"PcrKey\", how=\"left\")\n",
    "events_df = events_df.merge(team_df, on=\"PcrKey\", how=\"left\")\n",
    "\n",
    "print(\"Destination-related features merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05e8c89f-2bce-4a9b-a789-f98e17e208c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Vitals: 1648it [04:38,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vitals merged. New shape: (297762, 82)\n"
     ]
    }
   ],
   "source": [
    "# Load and aggregate vitals from FACTPCRVITAL.txt\n",
    "\n",
    "vitals_path = \"../data/raw/FACTPCRVITAL.txt\"\n",
    "vitals_chunks = []\n",
    "\n",
    "# Vital fields of interest and new names\n",
    "vital_fields = {\n",
    "    \"eVitals_10\": \"heart_rate\",\n",
    "    \"eVitals_14\": \"resp_rate\",\n",
    "    \"eVitals_06\": \"systolic_bp\",\n",
    "    \"eVitals_12\": \"spo2\",\n",
    "    \"eVitals_18\": \"bgl\",\n",
    "    \"eVitals_16\": \"etco2\"\n",
    "}\n",
    "\n",
    "with open(vitals_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Vitals\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk = chunk[[\"PcrKey\"] + list(vital_fields.keys())]\n",
    "        chunk = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        vitals_chunks.append(chunk)\n",
    "\n",
    "vitals_df = pd.concat(vitals_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate: take the first non-null value per PcrKey per vital\n",
    "agg_dict = {col: \"first\" for col in vital_fields.keys()}\n",
    "vitals_agg = (\n",
    "    vitals_df.groupby(\"PcrKey\")\n",
    "    .agg(agg_dict)\n",
    "    .rename(columns=vital_fields)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge into main dataset\n",
    "events_df = events_df.merge(vitals_agg, on=\"PcrKey\", how=\"left\")\n",
    "print(\"Vitals merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4c7dc0a-db89-4c29-9950-354a4d1e9b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Medications: 628it [01:44,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medications merged. New shape: (297762, 85)\n"
     ]
    }
   ],
   "source": [
    "# Load and process medication administration data\n",
    "\n",
    "med_path = \"../data/raw/FACTPCRMEDICATION.txt\"\n",
    "med_chunks = []\n",
    "\n",
    "with open(med_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Medications\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eMedications_03\"] = chunk[\"eMedications_03\"].str.strip(\" ~'\")\n",
    "        chunk[\"eMedications_03Descr\"] = chunk[\"eMedications_03Descr\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            med_chunks.append(filtered[[\"PcrKey\", \"eMedications_03\", \"eMedications_03Descr\"]])\n",
    "\n",
    "med_df = pd.concat(med_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate\n",
    "med_agg = (\n",
    "    med_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        any_med_administered=(\"eMedications_03\", lambda x: 1),\n",
    "        med_count=(\"eMedications_03\", \"nunique\"),\n",
    "        naloxone_administered=(\"eMedications_03Descr\", lambda x: int(x.str.contains(\"naloxone|narcan\", case=False, na=False).any()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge\n",
    "events_df = events_df.merge(med_agg, on=\"PcrKey\", how=\"left\")\n",
    "events_df[[\"any_med_administered\", \"med_count\", \"naloxone_administered\"]] = events_df[[\"any_med_administered\", \"med_count\", \"naloxone_administered\"]].fillna(0).astype(int)\n",
    "\n",
    "print(\"Medications merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6140ae72-3ef4-4282-987c-a6bd689c503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Procedures: 934it [01:41,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedures merged. New shape: (297762, 87)\n"
     ]
    }
   ],
   "source": [
    "# Load and process procedure data\n",
    "\n",
    "proc_path = \"../data/raw/FACTPCRPROCEDURE.txt\"\n",
    "proc_chunks = []\n",
    "\n",
    "with open(proc_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Procedures\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~'\")\n",
    "        chunk[\"eProcedures_03\"] = chunk[\"eProcedures_03\"].str.strip(\" ~'\")\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(events_df[\"PcrKey\"])]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            proc_chunks.append(filtered[[\"PcrKey\", \"eProcedures_03\"]])\n",
    "\n",
    "proc_df = pd.concat(proc_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate\n",
    "proc_agg = (\n",
    "    proc_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        any_procedure=(\"eProcedures_03\", lambda x: 1),\n",
    "        proc_count=(\"eProcedures_03\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge\n",
    "events_df = events_df.merge(proc_agg, on=\"PcrKey\", how=\"left\")\n",
    "events_df[[\"any_procedure\", \"proc_count\"]] = events_df[[\"any_procedure\", \"proc_count\"]].fillna(0).astype(int)\n",
    "\n",
    "print(\"Procedures merged. New shape:\", events_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af268d-8e4d-4710-95f9-66138dfad62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc657c-196d-4701-866a-c21831744ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8d511-8e6f-45b1-84d6-f2517dc7360f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9cdf8-bec5-4192-a216-18a35ada2761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb154309-1a19-4965-9b88-d16d6998ac52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae40da-449a-4bad-a6c3-65cbb670f43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d3ad806-9565-4be8-8a8c-729a880d451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Cause of Injury: 545it [00:34, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cause of Injury records loaded: 271756\n"
     ]
    }
   ],
   "source": [
    "cause_injury_path = \"../data/raw/FACTPCRCAUSEOFINJURY.txt\"\n",
    "\n",
    "cause_injury_chunks = []\n",
    "\n",
    "with open(cause_injury_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Cause of Injury\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        \n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            cause_injury_chunks.append(filtered)\n",
    "\n",
    "cause_injury_df = pd.concat(cause_injury_chunks, ignore_index=True)\n",
    "print(\"Cause of Injury records loaded:\", len(cause_injury_df))\n",
    "\n",
    "cause_injury_agg = (\n",
    "    cause_injury_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        cause_injury_first=(\"eInjury_01\", \"first\"),\n",
    "        cause_injury_count=(\"eInjury_01\", \"count\"),\n",
    "        unique_causes=(\"eInjury_01\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c3d9f5-d136-4da7-8259-420c3a6dc189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in ../data/raw/FACTPCRPRIMARYSYMPTOM.txt:\n",
      "- PcrKey\n",
      "- eSituation_09\n",
      "Columns in ../data/raw/FACTPCRADDITIONALSYMPTOM.txt:\n",
      "- PcrKey\n",
      "- eSituation_10\n",
      "Columns in ../data/raw/FACTPCRCAUSEOFINJURY.txt:\n",
      "- PcrKey\n",
      "- eInjury_01\n",
      "Columns in ../data/raw/FACTPCRINJURYRISKFACTOR.txt:\n",
      "- PcrKey\n",
      "- eInjury_04\n",
      "Columns in ../data/raw/FACTPCRTRAUMACRITERIA.txt:\n",
      "- PcrKey\n",
      "- eInjury_03\n",
      "Columns in ../data/raw/FACTPCRBARRIERTOCARE.txt:\n",
      "- PcrKey\n",
      "- eHistory_01\n",
      "Columns in ../data/raw/FACTPCRDISPATCHDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_08\n",
      "Columns in ../data/raw/FACTPCRRESPONSEDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_09\n",
      "Columns in ../data/raw/FACTPCRSCENEDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_10\n",
      "Columns in ../data/raw/FACTPCRTRANSPORTDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_11\n",
      "Columns in ../data/raw/FACTPCRTURNAROUNDDELAY.txt:\n",
      "- PcrKey\n",
      "- eResponse_12\n",
      "Columns in ../data/raw/FACTPCRDESTINATIONREASON.txt:\n",
      "- PcrKey\n",
      "- eDisposition_20\n",
      "Columns in ../data/raw/FACTPCRDESTINATIONTEAM.txt:\n",
      "- eDisposition_25\n",
      "- PcrKey\n",
      "- eDisposition_24\n",
      "Columns in ../data/raw/FACTPCRWORKRELATEDEXPOSURE.txt:\n",
      "- PcrKey\n",
      "- eOther_05\n"
     ]
    }
   ],
   "source": [
    "def inspect_file_columns(file_path, n_rows=5):\n",
    "    \"\"\"\n",
    "    Quickly read the first few rows of a pipe-delimited file\n",
    "    and print cleaned column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        delimiter=\"|\",\n",
    "        nrows=n_rows,\n",
    "        dtype=str\n",
    "    )\n",
    "    # Clean columns\n",
    "    df.columns = df.columns.str.strip(\" ~'\")\n",
    "    print(f\"Columns in {file_path}:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}\")\n",
    "\n",
    "# Symptoms\n",
    "inspect_file_columns(\"../data/raw/FACTPCRPRIMARYSYMPTOM.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRADDITIONALSYMPTOM.txt\")\n",
    "\n",
    "# Cause of injury\n",
    "inspect_file_columns(\"../data/raw/FACTPCRCAUSEOFINJURY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRINJURYRISKFACTOR.txt\")\n",
    "\n",
    "# Trauma criteria\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTRAUMACRITERIA.txt\")\n",
    "\n",
    "# Barriers to care\n",
    "inspect_file_columns(\"../data/raw/FACTPCRBARRIERTOCARE.txt\")\n",
    "\n",
    "# Response and transport delays\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDISPATCHDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRRESPONSEDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRSCENEDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTRANSPORTDELAY.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRTURNAROUNDDELAY.txt\")\n",
    "\n",
    "# Destination details\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDESTINATIONREASON.txt\")\n",
    "inspect_file_columns(\"../data/raw/FACTPCRDESTINATIONTEAM.txt\")\n",
    "\n",
    "# Work related exposure\n",
    "inspect_file_columns(\"../data/raw/FACTPCRWORKRELATEDEXPOSURE.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34e86d6a-3c45-4440-8af5-ff21e147a0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Injury Risk Factor: 543it [00:35, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injury Risk Factor records loaded: 271206\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['eInjury_03'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m injury_risk_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(injury_risk_chunks, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInjury Risk Factor records loaded:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(injury_risk_df))\n\u001b[1;32m     20\u001b[0m injury_risk_agg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 21\u001b[0m     \u001b[43minjury_risk_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPcrKey\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43minjury_risk_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meInjury_03\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43munique_injury_risks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meInjury_03\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnunique\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[1;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[1;32m   1606\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1607\u001b[0m ):\n\u001b[0;32m-> 1608\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1611\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/apply.py:462\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m is_groupby \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[1;32m    461\u001b[0m func \u001b[38;5;241m=\u001b[39m cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[0;32m--> 462\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m is_non_unique_col \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    465\u001b[0m     selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(selected_obj\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nemsis-env/lib/python3.10/site-packages/pandas/core/apply.py:663\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m     cols \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;241m.\u001b[39mdifference(obj\u001b[38;5;241m.\u001b[39mcolumns, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    665\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['eInjury_03'] do not exist\""
     ]
    }
   ],
   "source": [
    "injury_risk_path = \"../data/raw/FACTPCRINJURYRISKFACTOR.txt\"\n",
    "\n",
    "injury_risk_chunks = []\n",
    "\n",
    "with open(injury_risk_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Injury Risk Factor\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        \n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            injury_risk_chunks.append(filtered)\n",
    "\n",
    "injury_risk_df = pd.concat(injury_risk_chunks, ignore_index=True)\n",
    "print(\"Injury Risk Factor records loaded:\", len(injury_risk_df))\n",
    "\n",
    "injury_risk_agg = (\n",
    "    injury_risk_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        injury_risk_count=(\"eInjury_03\", \"count\"),\n",
    "        unique_injury_risks=(\"eInjury_03\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ae4c5b-f04a-4d95-9fae-0cbbf09183f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Event Records: 542it [04:33,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event records loaded: 271206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to event file\n",
    "event_path = \"../data/raw/pub_pcrevents_cp25.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "event_chunks = []\n",
    "\n",
    "with open(event_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Event Records\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            event_chunks.append(filtered)\n",
    "\n",
    "event_df = pd.concat(event_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Event records loaded:\", len(event_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf5fb5b6-6df8-41a1-8a79-08b7040ff065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Vitals: 1648it [05:29,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vitals aggregated: (271071, 64)\n"
     ]
    }
   ],
   "source": [
    "# Path to Vitals file\n",
    "vitals_path = \"../data/raw/FACTPCRVITAL.txt\"\n",
    "\n",
    "# Define vitals columns to extract and aggregate\n",
    "vital_cols = {\n",
    "    \"HeartRate\": \"eVitals_10\",\n",
    "    \"RespRate\": \"eVitals_14\",\n",
    "    \"SystolicBP\": \"eVitals_06\",\n",
    "    \"SpO2\": \"eVitals_12\",\n",
    "    \"BGL\": \"eVitals_18\",\n",
    "    \"ETCO2\": \"eVitals_16\",\n",
    "    \"GCS_Eye\": \"eVitals_19\",\n",
    "    \"GCS_Verbal\": \"eVitals_20\",\n",
    "    \"GCS_Motor\": \"eVitals_21\"\n",
    "}\n",
    "\n",
    "# Prepare chunks\n",
    "vitals_chunks = []\n",
    "\n",
    "with open(vitals_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Vitals\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)].copy()\n",
    "        if not filtered.empty:\n",
    "            for name, col in vital_cols.items():\n",
    "                if col in filtered.columns:\n",
    "                    extracted = filtered[col].str.extract(r\"(\\d+\\.?\\d*)\")[0]\n",
    "                    filtered[name] = pd.to_numeric(extracted, errors=\"coerce\").where(lambda x: x < 1000)\n",
    "            vitals_chunks.append(filtered[[\"PcrKey\"] + list(vital_cols.keys())])\n",
    "\n",
    "# Combine all\n",
    "vitals_df = pd.concat(vitals_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate per PcrKey\n",
    "vitals_agg = (\n",
    "    vitals_df.groupby(\"PcrKey\")[list(vital_cols.keys())]\n",
    "    .agg([\"first\", \"last\", \"min\", \"max\", \"mean\", \"std\", \"count\"])\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "vitals_agg.columns = [\"_\".join(col).strip() for col in vitals_agg.columns.values]\n",
    "vitals_agg = vitals_agg.reset_index()\n",
    "\n",
    "print(\"Vitals aggregated:\", vitals_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "573bb529-9762-48d4-aba6-83521c814b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Medications: 628it [01:44,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medications records loaded: 456070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Medications file\n",
    "meds_path = \"../data/raw/FACTPCRMEDICATION.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "meds_chunks = []\n",
    "\n",
    "with open(meds_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Medications\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        chunk[\"eMedications_03Descr\"] = chunk[\"eMedications_03Descr\"].str.strip().str.lower()\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            meds_chunks.append(filtered)\n",
    "\n",
    "# Combine all\n",
    "meds_df = pd.concat(meds_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Medications records loaded:\", len(meds_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2338e545-3a8d-426c-9e8d-a213e1b65b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medications aggregated: (271206, 7)\n"
     ]
    }
   ],
   "source": [
    "# Flag Naloxone\n",
    "naloxone_flag = meds_df[\"eMedications_03Descr\"].str.contains(\"naloxone|narcan\", na=False)\n",
    "\n",
    "# Aggregate medication info\n",
    "meds_agg = (\n",
    "    meds_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        total_meds=(\"eMedications_03\", \"count\"),\n",
    "        unique_meds=(\"eMedications_03\", \"nunique\"),\n",
    "        naloxone_doses=(\"eMedications_03Descr\", lambda x: x.str.contains(\"naloxone|narcan\", na=False).sum()),\n",
    "        naloxone_flag=(\"eMedications_03Descr\", lambda x: x.str.contains(\"naloxone|narcan\", na=False).any()),\n",
    "        first_route=(\"eMedications_07\", \"first\"),\n",
    "        first_response=(\"eMedications_10\", \"first\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Medications aggregated:\", meds_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e52deeba-5bdf-4ed7-be91-06c0c7523582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Procedures: 934it [01:50,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedures records loaded: 587755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Procedures file\n",
    "proc_path = \"../data/raw/FACTPCRPROCEDURE.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "proc_chunks = []\n",
    "\n",
    "with open(proc_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Procedures\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            proc_chunks.append(filtered)\n",
    "\n",
    "# Combine all\n",
    "proc_df = pd.concat(proc_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Procedures records loaded:\", len(proc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b630d80f-d28f-4696-96e4-5fdb2b0b252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedures aggregated: (271206, 6)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate procedures per PcrKey\n",
    "proc_agg = (\n",
    "    proc_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        procedure_count=(\"eProcedures_03\", \"count\"),\n",
    "        unique_procedures=(\"eProcedures_03\", \"nunique\"),\n",
    "        first_procedure=(\"eProcedures_03\", \"first\"),\n",
    "        all_procedures=(\"eProcedures_03\", lambda x: list(x.dropna()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Optionally stringify list for easier storage\n",
    "proc_agg[\"all_procedures_str\"] = proc_agg[\"all_procedures\"].apply(lambda x: \"|\".join(x) if x else \"\")\n",
    "\n",
    "print(\"Procedures aggregated:\", proc_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10e34f16-38cb-4411-afcf-4abba7e21559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading CPR Records: 546it [00:34, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPR records loaded: 275322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to CPR records\n",
    "cpr_path = \"../data/raw/FACTPCRARRESTCPRPROVIDED.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "cpr_chunks = []\n",
    "\n",
    "with open(cpr_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading CPR Records\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            cpr_chunks.append(filtered)\n",
    "\n",
    "cpr_df = pd.concat(cpr_chunks, ignore_index=True)\n",
    "\n",
    "print(\"CPR records loaded:\", len(cpr_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddafba95-d190-4349-924a-91adfecc02d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPR aggregated: (271206, 4)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate CPR info\n",
    "cpr_agg = (\n",
    "    cpr_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        cpr_given=(\"eArrest_09\", lambda x: True),\n",
    "        bystander_cpr=(\"eArrest_09\", lambda x: x.str.contains(\"BYSTANDER\", case=False, na=False).any()),\n",
    "        ems_cpr=(\"eArrest_09\", lambda x: x.str.contains(\"EMS|CREW|PROVIDER\", case=False, na=False).any())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"CPR aggregated:\", cpr_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9445c5cf-04dd-42aa-8adf-628c21fabebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ROSC Records: 543it [00:35, 15.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSC records loaded: 271712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to ROSC records\n",
    "rosc_path = \"../data/raw/FACTPCRARRESTROSC.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "rosc_chunks = []\n",
    "\n",
    "with open(rosc_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading ROSC Records\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            rosc_chunks.append(filtered)\n",
    "\n",
    "rosc_df = pd.concat(rosc_chunks, ignore_index=True)\n",
    "\n",
    "print(\"ROSC records loaded:\", len(rosc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c1db0a-badb-4f3a-a9ea-2bc5f2cf3935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSC aggregated: (271206, 2)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate ROSC flag\n",
    "rosc_agg = (\n",
    "    rosc_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        rosc_achieved=(\"eArrest_12\", lambda x: x.str.contains(\"YES\", case=False, na=False).any())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"ROSC aggregated:\", rosc_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fd3b1a3-faf6-4545-967e-51d5c8c3f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Computed Demographics: 542it [01:13,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics aggregated: (271206, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Computed Elements\n",
    "computed_path = \"../data/raw/ComputedElements.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "computed_chunks = []\n",
    "\n",
    "with open(computed_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Computed Demographics\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        # Check which columns exist\n",
    "        cols_available = [col for col in [\"PcrKey\", \"ePatient_15\", \"ePatient_16\"] if col in chunk.columns]\n",
    "        if not cols_available:\n",
    "            continue\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)][cols_available]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            computed_chunks.append(filtered)\n",
    "\n",
    "# Combine\n",
    "computed_df = pd.concat(computed_chunks, ignore_index=True)\n",
    "\n",
    "# Deduplicate\n",
    "demographics_agg = computed_df.drop_duplicates(subset=\"PcrKey\").copy()\n",
    "\n",
    "# Convert types if columns exist\n",
    "if \"ePatient_15\" in demographics_agg.columns:\n",
    "    demographics_agg[\"ePatient_15\"] = pd.to_numeric(demographics_agg[\"ePatient_15\"], errors=\"coerce\")\n",
    "\n",
    "if \"ePatient_16\" in demographics_agg.columns:\n",
    "    demographics_agg[\"ePatient_16\"] = demographics_agg[\"ePatient_16\"].astype(str).str.strip(\" ~\")\n",
    "\n",
    "print(\"Demographics aggregated:\", demographics_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "252fa5d1-e372-48bf-8b33-298b2b812a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Alcohol/Drug Use: 553it [00:34, 16.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol/Drug Use records loaded: 319541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Alcohol/Drug Use indicators\n",
    "drug_path = \"../data/raw/FACTPCRALCOHOLDRUGUSEINDICATOR.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "drug_chunks = []\n",
    "\n",
    "with open(drug_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Alcohol/Drug Use\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            drug_chunks.append(filtered)\n",
    "\n",
    "drug_df = pd.concat(drug_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Alcohol/Drug Use records loaded:\", len(drug_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e91a7434-96ba-4daa-87ab-436398b09673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol/Drug Use aggregated: (271206, 5)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate flags per PcrKey\n",
    "drug_agg = (\n",
    "    drug_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        use_flag_count=(\"eHistory_17\", \"count\"),\n",
    "        unique_use_flags=(\"eHistory_17\", \"nunique\"),\n",
    "        all_use_flags=(\"eHistory_17\", lambda x: list(x.dropna()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Stringify list for storage\n",
    "drug_agg[\"all_use_flags_str\"] = drug_agg[\"all_use_flags\"].apply(lambda x: \"|\".join(x) if x else \"\")\n",
    "\n",
    "print(\"Alcohol/Drug Use aggregated:\", drug_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7691d31a-ad71-45f8-a480-1d46bdf7e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Additional Symptoms: 580it [00:34, 16.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Symptoms records loaded: 306286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Additional Symptoms\n",
    "symptom_path = \"../data/raw/FACTPCRADDITIONALSYMPTOM.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "symptom_chunks = []\n",
    "\n",
    "with open(symptom_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Additional Symptoms\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            symptom_chunks.append(filtered)\n",
    "\n",
    "symptom_df = pd.concat(symptom_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Additional Symptoms records loaded:\", len(symptom_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc86e4f9-e8a8-413b-9db6-c6281a8f231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Symptoms aggregated: (267904, 5)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate per PcrKey\n",
    "symptom_agg = (\n",
    "    symptom_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        symptom_count=(\"eSituation_10\", \"count\"),\n",
    "        unique_symptoms=(\"eSituation_10\", \"nunique\"),\n",
    "        all_symptoms=(\"eSituation_10\", lambda x: list(x.dropna()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Stringify list for easier storage\n",
    "symptom_agg[\"all_symptoms_str\"] = symptom_agg[\"all_symptoms\"].apply(lambda x: \"|\".join(x) if x else \"\")\n",
    "\n",
    "print(\"Additional Symptoms aggregated:\", symptom_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c86ff5-4f92-4e3e-9c91-19275247c6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged vitals: now shape (271206, 110)\n",
      "Merged medications: now shape (271206, 116)\n",
      "Merged procedures: now shape (271206, 121)\n",
      "Merged cpr: now shape (271206, 124)\n",
      "Merged rosc: now shape (271206, 125)\n",
      "Merged demographics: now shape (271206, 125)\n",
      "Merged alcohol_drug_use: now shape (271206, 129)\n",
      "Merged additional_symptoms: now shape (271206, 133)\n",
      "All data merged and saved to opioid_cases_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Start with the Event DataFrame (1 row per PcrKey)\n",
    "df_merged = event_df.copy()\n",
    "\n",
    "# List of all aggregated DataFrames to merge\n",
    "feature_tables = {\n",
    "    \"vitals\": vitals_agg,\n",
    "    \"medications\": meds_agg,\n",
    "    \"procedures\": proc_agg,\n",
    "    \"cpr\": cpr_agg,\n",
    "    \"rosc\": rosc_agg,\n",
    "    \"demographics\": demographics_agg,\n",
    "    \"alcohol_drug_use\": drug_agg,\n",
    "    \"additional_symptoms\": symptom_agg\n",
    "}\n",
    "\n",
    "# Merge each\n",
    "for name, table in feature_tables.items():\n",
    "    df_merged = df_merged.merge(table, on=\"PcrKey\", how=\"left\")\n",
    "    print(f\"Merged {name}: now shape {df_merged.shape}\")\n",
    "\n",
    "# Save to CSV\n",
    "df_merged.to_csv(\"../data/interim/opioid_cases_full.csv\", index=False)\n",
    "\n",
    "print(\"All data merged and saved to opioid_cases_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ceaaf-4b73-4f6e-93e1-545fcaec81da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemsis-env",
   "language": "python",
   "name": "nemsis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
