{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2273e787-23cb-4c54-a406-a1f313207ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67456394-cd26-41fc-af0b-399bd364463b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Primary Impressions: 530it [00:24, 21.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary impressions matched: 231354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define target ICD-10 prefixes\n",
    "target_prefixes = (\"T40\", \"F11\")\n",
    "\n",
    "# Path to primary impressions file\n",
    "primary_path = \"../data/raw/FACTPCRPRIMARYIMPRESSION.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "primary_chunks = []\n",
    "\n",
    "with open(primary_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Processing Primary Impressions\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"eSituation_11\"] = chunk[\"eSituation_11\"].str.strip(\" ~\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        mask = chunk[\"eSituation_11\"].str.startswith(target_prefixes)\n",
    "        filtered = chunk.loc[mask, [\"PcrKey\"]]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            primary_chunks.append(filtered)\n",
    "\n",
    "primary_df = pd.concat(primary_chunks, ignore_index=True)\n",
    "print(\"Primary impressions matched:\", len(primary_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d367fc-6922-4a93-8705-3648ae4a8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Secondary Impressions: 544it [00:23, 23.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secondary impressions matched: 42853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to secondary impressions file\n",
    "secondary_path = \"../data/raw/FACTPCRSECONDARYIMPRESSION.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "secondary_chunks = []\n",
    "\n",
    "with open(secondary_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Processing Secondary Impressions\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"eSituation_12\"] = chunk[\"eSituation_12\"].str.strip(\" ~\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        mask = chunk[\"eSituation_12\"].str.startswith(target_prefixes)\n",
    "        filtered = chunk.loc[mask, [\"PcrKey\"]]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            secondary_chunks.append(filtered)\n",
    "\n",
    "secondary_df = pd.concat(secondary_chunks, ignore_index=True)\n",
    "print(\"Secondary impressions matched:\", len(secondary_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d3dd6f-00e4-4c9e-b51a-2bf8a2f9b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Primary Symptom: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pcr_key_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m chunk\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ~\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPcrKey\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPcrKey\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ~\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m filtered \u001b[38;5;241m=\u001b[39m chunk[chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPcrKey\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(\u001b[43mpcr_key_set\u001b[49m)]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     16\u001b[0m     primary_symptom_chunks\u001b[38;5;241m.\u001b[39mappend(filtered)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pcr_key_set' is not defined"
     ]
    }
   ],
   "source": [
    "# # Path to Primary Symptom\n",
    "# primary_symptom_path = \"../data/raw/FACTPCRPRIMARYSYMPTOM.txt\"\n",
    "\n",
    "# primary_symptom_chunks = []\n",
    "\n",
    "# with open(primary_symptom_path, \"r\") as f:\n",
    "#     for chunk in tqdm(\n",
    "#         pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "#         desc=\"Loading Primary Symptom\"\n",
    "#     ):\n",
    "#         chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "#         chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        \n",
    "#         filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "#         if not filtered.empty:\n",
    "#             primary_symptom_chunks.append(filtered)\n",
    "\n",
    "# primary_symptom_df = pd.concat(primary_symptom_chunks, ignore_index=True)\n",
    "# print(\"Primary Symptom records loaded:\", len(primary_symptom_df))\n",
    "\n",
    "# primary_symptom_agg = (\n",
    "#     primary_symptom_df.groupby(\"PcrKey\")\n",
    "#     .agg(\n",
    "#         primary_symptom_first=(\"eSituation_09\", \"first\"),\n",
    "#         primary_symptom_count=(\"eSituation_09\", \"count\"),\n",
    "#         unique_primary_symptoms=(\"eSituation_09\", \"nunique\")\n",
    "#     )\n",
    "#     .reset_index()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a8a9f6-cb7e-4247-877d-0239b0cf112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load SECONDARY impressions\n",
    "# secondary_uri = \"s3://{}/raw-data/FACTPCRSECONDARYIMPRESSION.txt\".format(credentials.BUCKET_NAME)\n",
    "# secondary_chunks = []\n",
    "\n",
    "# with open(secondary_uri, transport_params=transport_params) as f:\n",
    "#     for chunk in tqdm(\n",
    "#         pd.read_csv(f, delimiter=\"|\", chunksize=100_000),\n",
    "#         desc=\"Processing Secondary Impressions\"\n",
    "#     ):\n",
    "#         chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "#         chunk[\"eSituation_12\"] = chunk[\"eSituation_12\"].astype(str).str.strip(\" ~\")\n",
    "#         chunk[\"PcrKey\"] = chunk[\"PcrKey\"].astype(str).str.strip(\" ~\")\n",
    "\n",
    "#         mask = chunk[\"eSituation_12\"].str.startswith(target_prefixes)\n",
    "#         filtered = chunk[mask]\n",
    "\n",
    "#         if not filtered.empty:\n",
    "#             secondary_chunks.append(filtered[[\"PcrKey\"]])\n",
    "\n",
    "# secondary_df = pd.concat(secondary_chunks, ignore_index=True)\n",
    "# print(\"Secondary impressions matched:\", len(secondary_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df26d24-82c6-4884-92cb-9ab3bb83dec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique opioid-related PcrKeys: 271206\n"
     ]
    }
   ],
   "source": [
    "# Combine primary and secondary\n",
    "opioid_pcr_df = pd.concat([primary_df, secondary_df]).drop_duplicates()\n",
    "\n",
    "print(\"Total unique opioid-related PcrKeys:\", len(opioid_pcr_df))\n",
    "\n",
    "# Convert to set for fast lookup\n",
    "pcr_key_set = set(opioid_pcr_df[\"PcrKey\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ce6c1f-4c54-4d84-84b3-9ccfb60b94c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Primary Symptom: 534it [00:33, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary Symptom records loaded: 267186\n"
     ]
    }
   ],
   "source": [
    "# Path to Primary Symptom\n",
    "primary_symptom_path = \"../data/raw/FACTPCRPRIMARYSYMPTOM.txt\"\n",
    "\n",
    "primary_symptom_chunks = []\n",
    "\n",
    "with open(primary_symptom_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Primary Symptom\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        \n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            primary_symptom_chunks.append(filtered)\n",
    "\n",
    "primary_symptom_df = pd.concat(primary_symptom_chunks, ignore_index=True)\n",
    "print(\"Primary Symptom records loaded:\", len(primary_symptom_df))\n",
    "\n",
    "primary_symptom_agg = (\n",
    "    primary_symptom_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        primary_symptom_first=(\"eSituation_09\", \"first\"),\n",
    "        primary_symptom_count=(\"eSituation_09\", \"count\"),\n",
    "        unique_primary_symptoms=(\"eSituation_09\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d3ad806-9565-4be8-8a8c-729a880d451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Cause of Injury: 545it [00:34, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cause of Injury records loaded: 271756\n"
     ]
    }
   ],
   "source": [
    "cause_injury_path = \"../data/raw/FACTPCRCAUSEOFINJURY.txt\"\n",
    "\n",
    "cause_injury_chunks = []\n",
    "\n",
    "with open(cause_injury_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Cause of Injury\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        \n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            cause_injury_chunks.append(filtered)\n",
    "\n",
    "cause_injury_df = pd.concat(cause_injury_chunks, ignore_index=True)\n",
    "print(\"Cause of Injury records loaded:\", len(cause_injury_df))\n",
    "\n",
    "cause_injury_agg = (\n",
    "    cause_injury_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        cause_injury_first=(\"eInjury_01\", \"first\"),\n",
    "        cause_injury_count=(\"eInjury_01\", \"count\"),\n",
    "        unique_causes=(\"eInjury_01\", \"nunique\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ae4c5b-f04a-4d95-9fae-0cbbf09183f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Event Records: 542it [04:33,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event records loaded: 271206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to event file\n",
    "event_path = \"../data/raw/pub_pcrevents_cp25.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "event_chunks = []\n",
    "\n",
    "with open(event_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Event Records\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            event_chunks.append(filtered)\n",
    "\n",
    "event_df = pd.concat(event_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Event records loaded:\", len(event_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf5fb5b6-6df8-41a1-8a79-08b7040ff065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Vitals: 1648it [05:29,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vitals aggregated: (271071, 64)\n"
     ]
    }
   ],
   "source": [
    "# Path to Vitals file\n",
    "vitals_path = \"../data/raw/FACTPCRVITAL.txt\"\n",
    "\n",
    "# Define vitals columns to extract and aggregate\n",
    "vital_cols = {\n",
    "    \"HeartRate\": \"eVitals_10\",\n",
    "    \"RespRate\": \"eVitals_14\",\n",
    "    \"SystolicBP\": \"eVitals_06\",\n",
    "    \"SpO2\": \"eVitals_12\",\n",
    "    \"BGL\": \"eVitals_18\",\n",
    "    \"ETCO2\": \"eVitals_16\",\n",
    "    \"GCS_Eye\": \"eVitals_19\",\n",
    "    \"GCS_Verbal\": \"eVitals_20\",\n",
    "    \"GCS_Motor\": \"eVitals_21\"\n",
    "}\n",
    "\n",
    "# Prepare chunks\n",
    "vitals_chunks = []\n",
    "\n",
    "with open(vitals_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Vitals\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)].copy()\n",
    "        if not filtered.empty:\n",
    "            for name, col in vital_cols.items():\n",
    "                if col in filtered.columns:\n",
    "                    extracted = filtered[col].str.extract(r\"(\\d+\\.?\\d*)\")[0]\n",
    "                    filtered[name] = pd.to_numeric(extracted, errors=\"coerce\").where(lambda x: x < 1000)\n",
    "            vitals_chunks.append(filtered[[\"PcrKey\"] + list(vital_cols.keys())])\n",
    "\n",
    "# Combine all\n",
    "vitals_df = pd.concat(vitals_chunks, ignore_index=True)\n",
    "\n",
    "# Aggregate per PcrKey\n",
    "vitals_agg = (\n",
    "    vitals_df.groupby(\"PcrKey\")[list(vital_cols.keys())]\n",
    "    .agg([\"first\", \"last\", \"min\", \"max\", \"mean\", \"std\", \"count\"])\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "vitals_agg.columns = [\"_\".join(col).strip() for col in vitals_agg.columns.values]\n",
    "vitals_agg = vitals_agg.reset_index()\n",
    "\n",
    "print(\"Vitals aggregated:\", vitals_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "573bb529-9762-48d4-aba6-83521c814b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Medications: 628it [01:44,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medications records loaded: 456070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Medications file\n",
    "meds_path = \"../data/raw/FACTPCRMEDICATION.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "meds_chunks = []\n",
    "\n",
    "with open(meds_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Medications\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "        chunk[\"eMedications_03Descr\"] = chunk[\"eMedications_03Descr\"].str.strip().str.lower()\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            meds_chunks.append(filtered)\n",
    "\n",
    "# Combine all\n",
    "meds_df = pd.concat(meds_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Medications records loaded:\", len(meds_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2338e545-3a8d-426c-9e8d-a213e1b65b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medications aggregated: (271206, 7)\n"
     ]
    }
   ],
   "source": [
    "# Flag Naloxone\n",
    "naloxone_flag = meds_df[\"eMedications_03Descr\"].str.contains(\"naloxone|narcan\", na=False)\n",
    "\n",
    "# Aggregate medication info\n",
    "meds_agg = (\n",
    "    meds_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        total_meds=(\"eMedications_03\", \"count\"),\n",
    "        unique_meds=(\"eMedications_03\", \"nunique\"),\n",
    "        naloxone_doses=(\"eMedications_03Descr\", lambda x: x.str.contains(\"naloxone|narcan\", na=False).sum()),\n",
    "        naloxone_flag=(\"eMedications_03Descr\", lambda x: x.str.contains(\"naloxone|narcan\", na=False).any()),\n",
    "        first_route=(\"eMedications_07\", \"first\"),\n",
    "        first_response=(\"eMedications_10\", \"first\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Medications aggregated:\", meds_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e52deeba-5bdf-4ed7-be91-06c0c7523582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Procedures: 934it [01:50,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedures records loaded: 587755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Procedures file\n",
    "proc_path = \"../data/raw/FACTPCRPROCEDURE.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "proc_chunks = []\n",
    "\n",
    "with open(proc_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Procedures\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            proc_chunks.append(filtered)\n",
    "\n",
    "# Combine all\n",
    "proc_df = pd.concat(proc_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Procedures records loaded:\", len(proc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b630d80f-d28f-4696-96e4-5fdb2b0b252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedures aggregated: (271206, 6)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate procedures per PcrKey\n",
    "proc_agg = (\n",
    "    proc_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        procedure_count=(\"eProcedures_03\", \"count\"),\n",
    "        unique_procedures=(\"eProcedures_03\", \"nunique\"),\n",
    "        first_procedure=(\"eProcedures_03\", \"first\"),\n",
    "        all_procedures=(\"eProcedures_03\", lambda x: list(x.dropna()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Optionally stringify list for easier storage\n",
    "proc_agg[\"all_procedures_str\"] = proc_agg[\"all_procedures\"].apply(lambda x: \"|\".join(x) if x else \"\")\n",
    "\n",
    "print(\"Procedures aggregated:\", proc_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10e34f16-38cb-4411-afcf-4abba7e21559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading CPR Records: 546it [00:34, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPR records loaded: 275322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to CPR records\n",
    "cpr_path = \"../data/raw/FACTPCRARRESTCPRPROVIDED.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "cpr_chunks = []\n",
    "\n",
    "with open(cpr_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading CPR Records\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            cpr_chunks.append(filtered)\n",
    "\n",
    "cpr_df = pd.concat(cpr_chunks, ignore_index=True)\n",
    "\n",
    "print(\"CPR records loaded:\", len(cpr_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddafba95-d190-4349-924a-91adfecc02d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPR aggregated: (271206, 4)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate CPR info\n",
    "cpr_agg = (\n",
    "    cpr_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        cpr_given=(\"eArrest_09\", lambda x: True),\n",
    "        bystander_cpr=(\"eArrest_09\", lambda x: x.str.contains(\"BYSTANDER\", case=False, na=False).any()),\n",
    "        ems_cpr=(\"eArrest_09\", lambda x: x.str.contains(\"EMS|CREW|PROVIDER\", case=False, na=False).any())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"CPR aggregated:\", cpr_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9445c5cf-04dd-42aa-8adf-628c21fabebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ROSC Records: 543it [00:35, 15.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSC records loaded: 271712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to ROSC records\n",
    "rosc_path = \"../data/raw/FACTPCRARRESTROSC.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "rosc_chunks = []\n",
    "\n",
    "with open(rosc_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading ROSC Records\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            rosc_chunks.append(filtered)\n",
    "\n",
    "rosc_df = pd.concat(rosc_chunks, ignore_index=True)\n",
    "\n",
    "print(\"ROSC records loaded:\", len(rosc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c1db0a-badb-4f3a-a9ea-2bc5f2cf3935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSC aggregated: (271206, 2)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate ROSC flag\n",
    "rosc_agg = (\n",
    "    rosc_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        rosc_achieved=(\"eArrest_12\", lambda x: x.str.contains(\"YES\", case=False, na=False).any())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"ROSC aggregated:\", rosc_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fd3b1a3-faf6-4545-967e-51d5c8c3f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Computed Demographics: 542it [01:13,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics aggregated: (271206, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Computed Elements\n",
    "computed_path = \"../data/raw/ComputedElements.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "computed_chunks = []\n",
    "\n",
    "with open(computed_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Computed Demographics\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        # Check which columns exist\n",
    "        cols_available = [col for col in [\"PcrKey\", \"ePatient_15\", \"ePatient_16\"] if col in chunk.columns]\n",
    "        if not cols_available:\n",
    "            continue\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)][cols_available]\n",
    "\n",
    "        if not filtered.empty:\n",
    "            computed_chunks.append(filtered)\n",
    "\n",
    "# Combine\n",
    "computed_df = pd.concat(computed_chunks, ignore_index=True)\n",
    "\n",
    "# Deduplicate\n",
    "demographics_agg = computed_df.drop_duplicates(subset=\"PcrKey\").copy()\n",
    "\n",
    "# Convert types if columns exist\n",
    "if \"ePatient_15\" in demographics_agg.columns:\n",
    "    demographics_agg[\"ePatient_15\"] = pd.to_numeric(demographics_agg[\"ePatient_15\"], errors=\"coerce\")\n",
    "\n",
    "if \"ePatient_16\" in demographics_agg.columns:\n",
    "    demographics_agg[\"ePatient_16\"] = demographics_agg[\"ePatient_16\"].astype(str).str.strip(\" ~\")\n",
    "\n",
    "print(\"Demographics aggregated:\", demographics_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "252fa5d1-e372-48bf-8b33-298b2b812a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Alcohol/Drug Use: 553it [00:34, 16.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol/Drug Use records loaded: 319541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Alcohol/Drug Use indicators\n",
    "drug_path = \"../data/raw/FACTPCRALCOHOLDRUGUSEINDICATOR.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "drug_chunks = []\n",
    "\n",
    "with open(drug_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Alcohol/Drug Use\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            drug_chunks.append(filtered)\n",
    "\n",
    "drug_df = pd.concat(drug_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Alcohol/Drug Use records loaded:\", len(drug_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e91a7434-96ba-4daa-87ab-436398b09673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol/Drug Use aggregated: (271206, 5)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate flags per PcrKey\n",
    "drug_agg = (\n",
    "    drug_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        use_flag_count=(\"eHistory_17\", \"count\"),\n",
    "        unique_use_flags=(\"eHistory_17\", \"nunique\"),\n",
    "        all_use_flags=(\"eHistory_17\", lambda x: list(x.dropna()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Stringify list for storage\n",
    "drug_agg[\"all_use_flags_str\"] = drug_agg[\"all_use_flags\"].apply(lambda x: \"|\".join(x) if x else \"\")\n",
    "\n",
    "print(\"Alcohol/Drug Use aggregated:\", drug_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7691d31a-ad71-45f8-a480-1d46bdf7e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Additional Symptoms: 580it [00:34, 16.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Symptoms records loaded: 306286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to Additional Symptoms\n",
    "symptom_path = \"../data/raw/FACTPCRADDITIONALSYMPTOM.txt\"\n",
    "\n",
    "# Prepare chunks\n",
    "symptom_chunks = []\n",
    "\n",
    "with open(symptom_path, \"r\") as f:\n",
    "    for chunk in tqdm(\n",
    "        pd.read_csv(f, delimiter=\"|\", chunksize=100_000, dtype=str),\n",
    "        desc=\"Loading Additional Symptoms\"\n",
    "    ):\n",
    "        chunk.columns = chunk.columns.str.strip(\" ~'\")\n",
    "        chunk[\"PcrKey\"] = chunk[\"PcrKey\"].str.strip(\" ~\")\n",
    "\n",
    "        filtered = chunk[chunk[\"PcrKey\"].isin(pcr_key_set)]\n",
    "        if not filtered.empty:\n",
    "            symptom_chunks.append(filtered)\n",
    "\n",
    "symptom_df = pd.concat(symptom_chunks, ignore_index=True)\n",
    "\n",
    "print(\"Additional Symptoms records loaded:\", len(symptom_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc86e4f9-e8a8-413b-9db6-c6281a8f231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Symptoms aggregated: (267904, 5)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate per PcrKey\n",
    "symptom_agg = (\n",
    "    symptom_df.groupby(\"PcrKey\")\n",
    "    .agg(\n",
    "        symptom_count=(\"eSituation_10\", \"count\"),\n",
    "        unique_symptoms=(\"eSituation_10\", \"nunique\"),\n",
    "        all_symptoms=(\"eSituation_10\", lambda x: list(x.dropna()))\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Stringify list for easier storage\n",
    "symptom_agg[\"all_symptoms_str\"] = symptom_agg[\"all_symptoms\"].apply(lambda x: \"|\".join(x) if x else \"\")\n",
    "\n",
    "print(\"Additional Symptoms aggregated:\", symptom_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c86ff5-4f92-4e3e-9c91-19275247c6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged vitals: now shape (271206, 110)\n",
      "Merged medications: now shape (271206, 116)\n",
      "Merged procedures: now shape (271206, 121)\n",
      "Merged cpr: now shape (271206, 124)\n",
      "Merged rosc: now shape (271206, 125)\n",
      "Merged demographics: now shape (271206, 125)\n",
      "Merged alcohol_drug_use: now shape (271206, 129)\n",
      "Merged additional_symptoms: now shape (271206, 133)\n",
      "All data merged and saved to opioid_cases_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Start with the Event DataFrame (1 row per PcrKey)\n",
    "df_merged = event_df.copy()\n",
    "\n",
    "# List of all aggregated DataFrames to merge\n",
    "feature_tables = {\n",
    "    \"vitals\": vitals_agg,\n",
    "    \"medications\": meds_agg,\n",
    "    \"procedures\": proc_agg,\n",
    "    \"cpr\": cpr_agg,\n",
    "    \"rosc\": rosc_agg,\n",
    "    \"demographics\": demographics_agg,\n",
    "    \"alcohol_drug_use\": drug_agg,\n",
    "    \"additional_symptoms\": symptom_agg\n",
    "}\n",
    "\n",
    "# Merge each\n",
    "for name, table in feature_tables.items():\n",
    "    df_merged = df_merged.merge(table, on=\"PcrKey\", how=\"left\")\n",
    "    print(f\"Merged {name}: now shape {df_merged.shape}\")\n",
    "\n",
    "# Save to CSV\n",
    "df_merged.to_csv(\"../data/interim/opioid_cases_full.csv\", index=False)\n",
    "\n",
    "print(\"All data merged and saved to opioid_cases_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ceaaf-4b73-4f6e-93e1-545fcaec81da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemsis-env",
   "language": "python",
   "name": "nemsis-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
